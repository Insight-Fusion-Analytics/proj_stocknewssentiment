{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ff4ec4-a367-45ae-93fc-faf97764664d",
   "metadata": {},
   "source": [
    "**\n",
    "\n",
    "Positive: happy, good, excellent, amazing, great, wonderful, fantastic, outstanding, superb, positive, strong, confident, promising, robust, solid, optimistic, encouraging, favorable, profit, success, growth, high, gain, rise, up, bullish, increase, peak, record-high, outperform, surge, rally, revenue, earnings, dividends, expansion, upgrade, investment, acquisition, innovation, wealth, prosperity, uptrend, rallying, skyrocketing, surge, breakout, rebound, resurgence, breakthrough, recovery, exceeding expectations, soaring, beating estimates, surpassing forecasts, buy, buy-back, top pick, outperform, bullish trend, strong demand, oversubscribed, buy recommendation, institutional buying, accumulation, positive guidance, share repurchase, capital appreciation, strong economy, low unemployment, surplus, GDP growth, high consumer confidence, tax cuts, market optimism, job creation, rising wages, stable inflation, expansionary policy, fiscal stimulus\n",
    "\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf9510b-d357-4e11-bbe0-9716b713c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching news for: SPG...\n",
      "Saved: news_articles\\2025-02-05_SPG.txt\n",
      "Saved: news_articles\\2025-02-05_SPG_1.txt\n",
      "Saved: news_articles\\2025-02-19_SPG.txt\n",
      "Saved: news_articles\\2025-02-16_SPG.txt\n",
      "Saved: news_articles\\2025-02-19_SPG_1.txt\n",
      "Failed to fetch OHLC data for SPG: Too Many Requests. Rate limited. Try after a while.\n",
      "Failed to fetch past 21 days OHLC data for SPG: Too Many Requests. Rate limited. Try after a while.\n",
      "\n",
      "Fetching news for: WELL...\n",
      "Saved: news_articles\\2025-02-12_WELL.txt\n",
      "Saved: news_articles\\2025-02-14_WELL.txt\n",
      "Saved: news_articles\\2025-02-19_WELL.txt\n",
      "Saved: news_articles\\2025-02-19_WELL_1.txt\n",
      "Saved: news_articles\\2025-02-19_WELL_2.txt\n",
      "Failed to fetch OHLC data for WELL: Too Many Requests. Rate limited. Try after a while.\n",
      "Failed to fetch past 21 days OHLC data for WELL: Too Many Requests. Rate limited. Try after a while.\n",
      "\n",
      "No OHLC data to save.\n",
      "\n",
      "No past 21 days OHLC data to save.\n",
      "\n",
      "News scraping and OHLC data fetching completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode, urlparse, parse_qs\n",
    "from datetime import datetime, timedelta\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Stocks to scrape news for\n",
    "stocks = [ \"SPG\", \"WELL\"\n",
    "    # \"XOM\", \"CVX\", \"COP\", \"SLB\", \"EOG\", \"OXY\", \"VLO\", \"MPC\", \"PSX\",\n",
    "    # \"DOW\", \"DD\", \"LIN\", \"APD\", \"SHW\", \"NEM\", \"FCX\", \"ALB\", \"IP\", \"ECL\",\n",
    "    # \"GE\", \"HON\", \"UNP\", \"BA\", \"MMM\", \"CAT\", \"LMT\", \"RTX\", \"NOC\", \"DE\",\n",
    "    # \"AMZN\", \"TSLA\", \"HD\", \"MCD\", \"NKE\", \"SBUX\", \"BKNG\", \"LOW\", \"TGT\", \"GM\"\n",
    "]\n",
    "\n",
    "GOOGLE_NEWS_URL = \"https://www.google.com/search\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "OUTPUT_DIR = \"news_articles\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "FETCH_LIMIT = 15\n",
    "MAX_VALID_ARTICLES = 5\n",
    "\n",
    "\n",
    "def fetch_full_article(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def clean_google_url(google_url):\n",
    "    parsed_url = urlparse(google_url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    return query_params.get(\"q\", [google_url])[0]\n",
    "\n",
    "\n",
    "def convert_relative_date(relative_date):\n",
    "    now = datetime.today()\n",
    "    if \"hour\" in relative_date:\n",
    "        num = int(relative_date.split()[0])\n",
    "        article_date = now - timedelta(hours=num)\n",
    "    elif \"day\" in relative_date:\n",
    "        num = int(relative_date.split()[0])\n",
    "        article_date = now - timedelta(days=num)\n",
    "    elif \"week\" in relative_date:\n",
    "        num = int(relative_date.split()[0])\n",
    "        article_date = now - timedelta(weeks=num)\n",
    "    elif \"minute\" in relative_date:\n",
    "        article_date = now\n",
    "    else:\n",
    "        article_date = now\n",
    "    return article_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def scrape_news(stock):\n",
    "    print(f\"\\nFetching news for: {stock}...\")\n",
    "    valid_articles = []\n",
    "    params = {\"q\": f\"{stock} stock news\", \"tbm\": \"nws\", \"hl\": \"en\", \"gl\": \"us\"}\n",
    "    url = f\"{GOOGLE_NEWS_URL}?{urlencode(params)}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch news for {stock}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles_fetched = 0\n",
    "\n",
    "    for article in soup.select(\".SoaBEf\"):\n",
    "        if articles_fetched >= FETCH_LIMIT:\n",
    "            break\n",
    "\n",
    "        title_element = article.select_one(\".nDgy9d\")\n",
    "        url_element = article.select_one(\"a\")\n",
    "        date_element = article.select_one(\".OSrXXb\")\n",
    "\n",
    "        title = title_element.text.strip() if title_element else \"No Title\"\n",
    "        url = url_element[\"href\"] if url_element else \"\"\n",
    "        raw_date = date_element.text.strip() if date_element else \"Unknown Date\"\n",
    "\n",
    "        if url.startswith(\"/url?\"):\n",
    "            url = clean_google_url(url)\n",
    "\n",
    "        full_article = fetch_full_article(url)\n",
    "        if not full_article.strip():\n",
    "            continue\n",
    "\n",
    "        valid_articles.append({\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"date\": convert_relative_date(raw_date),\n",
    "            \"full_article\": full_article,\n",
    "        })\n",
    "\n",
    "        articles_fetched += 1\n",
    "\n",
    "        if len(valid_articles) >= MAX_VALID_ARTICLES:\n",
    "            break\n",
    "\n",
    "    return valid_articles\n",
    "\n",
    "\n",
    "def save_articles(stock, articles):\n",
    "    saved_files = []\n",
    "\n",
    "    for i, article in enumerate(articles):\n",
    "        filename = f\"{article['date']}_{stock}.txt\"\n",
    "        filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "        # Handle duplicate file names (appending _1, _2, etc.)\n",
    "        counter = 1\n",
    "        while os.path.exists(filepath):\n",
    "            filepath = os.path.join(OUTPUT_DIR, f\"{article['date']}_{stock}_{counter}.txt\")\n",
    "            counter += 1\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Stock: {stock}\\n\")\n",
    "            f.write(f\"Date: {article['date']}\\n\")\n",
    "            f.write(f\"Title: {article['title']}\\n\")\n",
    "            f.write(f\"URL: {article['url']}\\n\")\n",
    "            f.write(f\"Full Article:\\n{article['full_article']}\\n\")\n",
    "\n",
    "        saved_files.append(filepath)\n",
    "        print(f\"Saved: {filepath}\")\n",
    "\n",
    "    return len(saved_files) > 0\n",
    "\n",
    "\n",
    "def fetch_recent_ohlc(stock):\n",
    "    try:\n",
    "        ticker = yf.Ticker(stock)\n",
    "        df = ticker.history(period=\"1d\")\n",
    "\n",
    "        if not df.empty:\n",
    "            data = {\n",
    "                \"Stock\": stock,\n",
    "                \"Open_Price\": df.iloc[-1][\"Open\"],\n",
    "                \"Close_Price\": df.iloc[-1][\"Close\"],\n",
    "                \"High_Price\": df.iloc[-1][\"High\"],\n",
    "                \"Low_Price\": df.iloc[-1][\"Low\"],\n",
    "                \"Volume\": df.iloc[-1][\"Volume\"],\n",
    "            }\n",
    "            print(f\"OHLC Data Fetched: {stock} - Close: {data['Close_Price']}\")\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"No OHLC data found for {stock}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch OHLC data for {stock}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_past_21_days_ohlc(stock):\n",
    "    try:\n",
    "        ticker = yf.Ticker(stock)\n",
    "        df = ticker.history(period=\"21d\")\n",
    "\n",
    "        if not df.empty:\n",
    "            df[\"Stock\"] = stock\n",
    "            df.reset_index(inplace=True)\n",
    "            df[\"Date\"] = df[\"Date\"].dt.tz_localize(None)  # Remove timezone\n",
    "            df = df[[\"Date\", \"Stock\", \"Open\", \"High\", \"Close\", \"Low\", \"Volume\"]]\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"No past 21 days OHLC data found for {stock}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch past 21 days OHLC data for {stock}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "all_ohlc_data = []\n",
    "past_21_days_ohlc_data = []\n",
    "\n",
    "for stock in stocks:\n",
    "    # 1. Scrape and Save News Articles\n",
    "    articles = scrape_news(stock)\n",
    "    saved_articles = save_articles(stock, articles) if articles else False\n",
    "\n",
    "    # 2. Fetch OHLC Data only if news was saved\n",
    "    if saved_articles:\n",
    "        ohlc_data = fetch_recent_ohlc(stock)\n",
    "        if ohlc_data:\n",
    "            all_ohlc_data.append(ohlc_data)\n",
    "\n",
    "        # 3. Fetch past 21 days OHLC Data\n",
    "        past_21_days_df = fetch_past_21_days_ohlc(stock)\n",
    "        if not past_21_days_df.empty:\n",
    "            past_21_days_ohlc_data.append(past_21_days_df)\n",
    "\n",
    "    # Pause between requests\n",
    "    time.sleep(random.uniform(5, 10))\n",
    "\n",
    "# Save OHLC Data to Excel\n",
    "if all_ohlc_data:\n",
    "    ohlc_df = pd.DataFrame(all_ohlc_data)\n",
    "    ohlc_df.to_excel(\"stock_ohlc_data.xlsx\", index=False)\n",
    "    print(\"\\nOHLC data saved to 'stock_ohlc_data_ex.xlsx'\")\n",
    "else:\n",
    "    print(\"\\nNo OHLC data to save.\")\n",
    "\n",
    "# Save Past 21 Days OHLC Data to Excel\n",
    "if past_21_days_ohlc_data:\n",
    "    past_21_days_df = pd.concat(past_21_days_ohlc_data, ignore_index=True)\n",
    "    past_21_days_df.to_excel(\"stock_ohlc_past_21_days.xlsx\", index=False)\n",
    "    print(\"\\nPast 21 days OHLC data saved to 'Stock_ohlc_past_21_days.xlsx'\")\n",
    "else:\n",
    "    print(\"\\nNo past 21 days OHLC data to save.\")\n",
    "\n",
    "print(\"\\nNews scraping and OHLC data fetching completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f13138aa-fa24-49c5-8aae-330726e90df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to filtered_output1.xlsx\n",
      "Final output with OHLC data saved to 'final_output_with_ohlc.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from datetime import datetime\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def perform_sentiment_analysis(article_text):\n",
    "    vader_scores = analyzer.polarity_scores(article_text)\n",
    "    return vader_scores['compound']\n",
    "\n",
    "def parse_date(date_string):\n",
    "    formats = ['%Y-%m-%d', '%d-%m-%Y', '%d/%m/%Y']\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_string, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    print(f\"Invalid date format: {date_string}\")\n",
    "    return None\n",
    "\n",
    "def assign_dynamic_weights(group):\n",
    "    max_date = group['Date'].max()\n",
    "    group['Time_Diff_Days'] = (max_date - group['Date']).dt.days\n",
    "\n",
    "    min_weight = 0.1\n",
    "    max_weight = 0.9\n",
    "    decay_factor = 0.5\n",
    "\n",
    "    group['Weight'] = max_weight * np.exp(-decay_factor * group['Time_Diff_Days'])\n",
    "    group['Weight'] = group['Weight'].clip(lower=min_weight, upper=max_weight)\n",
    "    group['Weight'] = group['Weight'] / group['Weight'].sum()\n",
    "\n",
    "    return group\n",
    "\n",
    "def process_files(input_folder, output_excel, ohlc_file, final_output_with_ohlc):\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(input_folder, filename), 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "                stock = date = title = url = None\n",
    "                full_article_lines = []\n",
    "                collecting_article = False\n",
    "\n",
    "                for line in lines:\n",
    "                    if line.startswith(\"Stock:\"):\n",
    "                        stock = line.split(\":\", 1)[1].strip()\n",
    "                    elif line.startswith(\"Date:\"):\n",
    "                        date = line.split(\":\", 1)[1].strip()\n",
    "                    elif line.startswith(\"Title:\"):\n",
    "                        title = line.split(\":\", 1)[1].strip()\n",
    "                    elif line.startswith(\"URL:\"):\n",
    "                        url = line.split(\":\", 1)[1].strip()\n",
    "                    elif line.startswith(\"Full Article:\"):\n",
    "                        collecting_article = True\n",
    "                    elif collecting_article:\n",
    "                        full_article_lines.append(line.strip())\n",
    "\n",
    "                full_article = \"\\n\".join(full_article_lines).strip()\n",
    "\n",
    "                if not full_article or len(full_article.split()) < 5:\n",
    "                    print(f\"Skipping file due to empty or short article: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                sentiment_score = perform_sentiment_analysis(full_article)\n",
    "\n",
    "                date = parse_date(date)\n",
    "                if not date:\n",
    "                    print(f\"Skipping file due to invalid date: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                if stock and date and title and url:\n",
    "                    data.append({\n",
    "                        'Date': date,\n",
    "                        'Stock': stock,\n",
    "                        'Title': title,\n",
    "                        'URL': url,\n",
    "                        'Sentiment_score': sentiment_score\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Skipping file due to missing fields: {filename}\")\n",
    "\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.sort_values(by=[\"Stock\", \"Date\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "        final_results = []\n",
    "\n",
    "        for stock, group in df.groupby('Stock'):\n",
    "            group = group.head(5).copy()\n",
    "            group = assign_dynamic_weights(group)\n",
    "\n",
    "            weighted_avg_score = (group['Sentiment_score'] * group['Weight']).sum()\n",
    "            sentiment_std = group['Sentiment_score'].std() or 0\n",
    "\n",
    "            if weighted_avg_score >= sentiment_std:\n",
    "                final_label_std = 'Positive'\n",
    "            elif weighted_avg_score <= -sentiment_std:\n",
    "                final_label_std = 'Negative'\n",
    "            else:\n",
    "                final_label_std = 'Neutral'\n",
    "\n",
    "            if weighted_avg_score > 0.4:\n",
    "                final_label_threshold = 'Positive'\n",
    "            elif weighted_avg_score < -0.4:\n",
    "                final_label_threshold = 'Negative'\n",
    "            else:\n",
    "                final_label_threshold = 'Neutral'\n",
    "\n",
    "            for _, row in group.iterrows():\n",
    "                final_results.append({\n",
    "                    'Date': row['Date'],\n",
    "                    'Stock': stock,\n",
    "                    'Title': row['Title'],\n",
    "                    'URL': row['URL'],\n",
    "                    'Positive_score': max(row['Sentiment_score'], 0),\n",
    "                    'Negative_score': abs(min(row['Sentiment_score'], 0)),\n",
    "                    'Sentiment_score': row['Sentiment_score'],\n",
    "                    'Weightage': row['Weight'],\n",
    "                    'Weighted_average_score': weighted_avg_score,\n",
    "                    'Final_sentiment_label_on_std': final_label_std,\n",
    "                    'Final_sentiment_label_on_threshold_value': final_label_threshold\n",
    "                })\n",
    "\n",
    "        final_df = pd.DataFrame(final_results)\n",
    "        final_df['Date'] = final_df['Date'].dt.strftime('%d %b %Y %H:%M')\n",
    "        final_df.to_excel(output_excel, index=False)\n",
    "\n",
    "        print(f\"Output saved to {output_excel}\")\n",
    "\n",
    "        # Load OHLC Data\n",
    "        ohlc_df = pd.read_excel(ohlc_file)\n",
    "\n",
    "        # Merge with Analysis Results on 'Stock' only\n",
    "        merged_df = pd.merge(final_df, ohlc_df, on='Stock', how='left')\n",
    "\n",
    "        # Fill missing values for each stock by forward and backward fill\n",
    "        merged_df[['Open_Price', 'Close_Price', 'High_Price', 'Low_Price', 'Volume']] = merged_df.groupby('Stock')[\n",
    "            ['Open_Price', 'Close_Price', 'High_Price', 'Low_Price', 'Volume']\n",
    "        ].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "        merged_df.to_excel(final_output_with_ohlc, index=False)\n",
    "        print(f\"Final output with OHLC data saved to '{final_output_with_ohlc}'\")\n",
    "\n",
    "    else:\n",
    "        print(\"No valid data to process.\")\n",
    "\n",
    "input_folder = 'news_articles'\n",
    "output_excel = 'filtered_output1.xlsx'\n",
    "ohlc_file = 'stock_ohlc_data.xlsx'\n",
    "final_output_with_ohlc = 'final_output_with_ohlc.xlsx'\n",
    "\n",
    "process_files(input_folder, output_excel, ohlc_file, final_output_with_ohlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701ca189-a440-4cbe-bc66-049038dba719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91815\\AppData\\Local\\Temp\\ipykernel_15872\\3263989602.py:63: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Stock', group_keys=False).apply(process_stock)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import talib\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_excel('stock_ohlc_past_21_days.xlsx')\n",
    "\n",
    "# Ensure Date is datetime type and sort by Stock and Date\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.sort_values(by=['Stock', 'Date'], inplace=True)\n",
    "\n",
    "# Indicators, Swing Highs/Lows, Stop Loss, Take Profit per stock\n",
    "def process_stock(stock_df):\n",
    "    # Indicators\n",
    "    stock_df['SMA9'] = talib.SMA(stock_df['Close'], timeperiod=9)\n",
    "    stock_df['SMA21'] = talib.SMA(stock_df['Close'], timeperiod=21)\n",
    "    stock_df['RSI14'] = talib.RSI(stock_df['Close'], timeperiod=14)\n",
    "\n",
    "    # Buy/Sell Signal\n",
    "    stock_df['Buy_Signal'] = (stock_df['SMA9'] > stock_df['SMA21']) & (stock_df['RSI14'] > 50)\n",
    "    stock_df['Sell_Signal'] = (stock_df['SMA9'] < stock_df['SMA21']) & (stock_df['RSI14'] < 50)\n",
    "\n",
    "    # Detect Swing Highs/Lows\n",
    "    def detect_swing_high(stock_df, window=3):\n",
    "        stock_df['Swing_High'] = np.nan\n",
    "        for i in range(window, len(stock_df) - window):\n",
    "            if stock_df['High'].iloc[i] == max(stock_df['High'].iloc[i - window: i + window + 1]):\n",
    "                stock_df.loc[stock_df.index[i], 'Swing_High'] = stock_df['High'].iloc[i]\n",
    "        return stock_df\n",
    "\n",
    "    def detect_swing_low(stock_df, window=3):\n",
    "        stock_df['Swing_Low'] = np.nan\n",
    "        for i in range(window, len(stock_df) - window):\n",
    "            if stock_df['Low'].iloc[i] == min(stock_df['Low'].iloc[i - window: i + window + 1]):\n",
    "                stock_df.loc[stock_df.index[i], 'Swing_Low'] = stock_df['Low'].iloc[i]\n",
    "        return stock_df\n",
    "\n",
    "    stock_df = detect_swing_high(stock_df)\n",
    "    stock_df = detect_swing_low(stock_df)\n",
    "\n",
    "    # Fill NAs in Swing High/Low\n",
    "    stock_df['Swing_High'] = stock_df['Swing_High'].ffill()\n",
    "    stock_df['Swing_Low'] = stock_df['Swing_Low'].ffill()\n",
    "\n",
    "    # Add Stop Loss based on Swing Levels\n",
    "    stock_df['Buy_Stop_Loss'] = stock_df.apply(lambda x: x['Swing_Low'] if x['Buy_Signal'] else np.nan, axis=1)\n",
    "    stock_df['Sell_Stop_Loss'] = stock_df.apply(lambda x: x['Swing_High'] if x['Sell_Signal'] else np.nan, axis=1)\n",
    "\n",
    "    # Risk-to-Reward Ratio for Take Profit\n",
    "    RRR = 2  # Example: Risk-to-Reward Ratio of 2:1\n",
    "\n",
    "    # Add Take Profit levels\n",
    "    stock_df['Buy_Take_Profit'] = stock_df.apply(\n",
    "        lambda x: x['Close'] + (x['Close'] - x['Buy_Stop_Loss']) * RRR if x['Buy_Signal'] else np.nan, axis=1\n",
    "    )\n",
    "    stock_df['Sell_Take_Profit'] = stock_df.apply(\n",
    "        lambda x: x['Close'] - (x['Sell_Stop_Loss'] - x['Close']) * RRR if x['Sell_Signal'] else np.nan, axis=1\n",
    "    )\n",
    "\n",
    "    return stock_df\n",
    "\n",
    "# Group by Stock and process each stock separately\n",
    "df = df.groupby('Stock', group_keys=False).apply(process_stock)\n",
    "\n",
    "# Extract the most recent data for each stock\n",
    "latest_data = df.loc[df.groupby('Stock')['Date'].idxmax()]\n",
    "\n",
    "# Save full data and latest data\n",
    "df.to_excel('output_with_swing_high_low_take_profit.xlsx', index=False)\n",
    "latest_data.to_excel('output_most_recent_data.xlsx', index=False)\n",
    "\n",
    "print(\"Processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a279b6c5-8f93-41a9-a9e3-0c1d210b5558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output with indicators merged successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the final sentiment analysis output\n",
    "analyzer_df = pd.read_excel('final_output_with_ohlc.xlsx')\n",
    "\n",
    "# Load the most recent stock data with indicators\n",
    "indicators_df = pd.read_excel('output_most_recent_data.xlsx')\n",
    "\n",
    "# Select only necessary columns from indicators data\n",
    "indicator_columns = [\n",
    "    'Stock', 'SMA9', 'SMA21', 'RSI14', 'Buy_Signal', 'Sell_Signal',\n",
    "    'Swing_High', 'Swing_Low', 'Buy_Stop_Loss', 'Sell_Stop_Loss',\n",
    "    'Buy_Take_Profit', 'Sell_Take_Profit'\n",
    "]\n",
    "\n",
    "indicators_df = indicators_df[indicator_columns]\n",
    "\n",
    "# Merge the two dataframes based on 'Stock'\n",
    "merged_df = pd.merge(analyzer_df, indicators_df, on='Stock', how='left')\n",
    "\n",
    "# Save the merged dataframe to a new Excel file\n",
    "merged_df.to_excel('final_output_with_indicators.xlsx', index=False)\n",
    "\n",
    "print(\"Final output with indicators merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab81757-eb74-4255-8f48-d3e423ccb230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
