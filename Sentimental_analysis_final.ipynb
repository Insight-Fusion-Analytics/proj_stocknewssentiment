{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75db59a7-60b6-4d95-8c28-8992502c0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import random\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# from datetime import datetime, timedelta\n",
    "# from urllib.parse import urlencode\n",
    "# import re\n",
    "\n",
    "# # Define the function to fetch full article content\n",
    "# def fetch_full_article(url):\n",
    "#     headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "#     response = requests.get(url, headers=headers)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     article_content = soup.find('div', {'class': 'article-body'}) or soup.find('div', {'class': 'content'}) or soup.find('article')\n",
    "#     if article_content:\n",
    "#         return article_content.get_text(strip=True)\n",
    "#     else:\n",
    "#         return \"Full content not available.\"\n",
    "\n",
    "# # Function to convert relative dates like '2 days ago' to full date\n",
    "# def convert_relative_date(relative_date):\n",
    "#     now = datetime.today()\n",
    "#     match = re.search(r'(\\d+)\\s*(hour|day|week|minute)s?\\s*ago', relative_date)\n",
    "#     if match:\n",
    "#         value, unit = int(match.group(1)), match.group(2)\n",
    "#         if unit == 'hour':\n",
    "#             return (now - timedelta(hours=value)).strftime('%Y-%m-%d')\n",
    "#         elif unit == 'day':\n",
    "#             return (now - timedelta(days=value)).strftime('%Y-%m-%d')\n",
    "#         elif unit == 'week':\n",
    "#             return (now - timedelta(weeks=value)).strftime('%Y-%m-%d')\n",
    "#         elif unit == 'minute':\n",
    "#             return now.strftime('%Y-%m-%d')\n",
    "#     return now.strftime('%Y-%m-%d')\n",
    "\n",
    "# # Define the function to fetch news for each stock\n",
    "# def fetch_news_for_stock(stock_symbol):\n",
    "#     news_articles = []\n",
    "#     date_ranges = [\"1d\", \"2d\", \"3d\", \"4d\", \"5d\"]\n",
    "\n",
    "#     for date_range in date_ranges:\n",
    "#         url = f\"https://www.google.com/search?q={stock_symbol}+stock+news&tbm=nws&tbs=qdr:{date_range}\"\n",
    "#         headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "#         response = requests.get(url, headers=headers)\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "#         articles = soup.find_all('div', class_='BVG0Nb')\n",
    "#         for article in articles:\n",
    "#             title = article.get_text()\n",
    "#             link = article.find('a')['href']\n",
    "#             raw_date = article.find('span', class_='OSrXXb').get_text() if article.find('span', class_='OSrXXb') else 'Unknown Date'\n",
    "#             date = convert_relative_date(raw_date)\n",
    "#             full_article = fetch_full_article(link)\n",
    "#             news_articles.append({\n",
    "#                 'stock': stock_symbol, \n",
    "#                 'date': date, \n",
    "#                 'title': title, \n",
    "#                 'url': link, \n",
    "#                 'full_article': full_article\n",
    "#             })\n",
    "\n",
    "#         # Add a delay to prevent rate limiting\n",
    "#         time.sleep(random.uniform(5, 15))\n",
    "\n",
    "#     return news_articles\n",
    "\n",
    "# # Define the stock dictionary\n",
    "# stock_dict = {\n",
    "#     \"Energy\": [\"XOM\", \"CVX\", \"COP\", \"SLB\", \"EOG\", \"OXY\", \"PXD\", \"VLO\", \"MPC\", \"PSX\"],\n",
    "#     \"Materials\": [\"DOW\", \"DD\", \"LIN\", \"APD\", \"SHW\", \"NEM\", \"FCX\", \"ALB\", \"IP\", \"ECL\"],\n",
    "#     \"Industrials\": [\"GE\", \"HON\", \"UNP\", \"BA\", \"MMM\", \"CAT\", \"LMT\", \"RTX\", \"NOC\", \"DE\"],\n",
    "#     \"Consumer Discretionary\": [\"AMZN\", \"TSLA\", \"HD\", \"MCD\", \"NKE\", \"SBUX\", \"BKNG\", \"LOW\", \"TGT\", \"GM\"],\n",
    "#     \"Consumer Staples\": [\"PG\", \"KO\", \"PEP\", \"WMT\", \"COST\", \"PM\", \"MO\", \"MDLZ\", \"CL\", \"KMB\"],\n",
    "#     \"Healthcare\": [\"JNJ\", \"PFE\", \"MRK\", \"ABBV\", \"UNH\", \"AMGN\", \"BMY\", \"GILD\", \"LLY\", \"ANTM\"],\n",
    "#     \"Financials\": [\"JPM\", \"BAC\", \"WFC\", \"C\", \"GS\", \"MS\", \"AXP\", \"USB\", \"PNC\", \"SCHW\"],\n",
    "#     \"Information Technology\": [\"AAPL\", \"MSFT\", \"GOOGL\", \"NVDA\", \"META\", \"INTC\", \"CSCO\", \"ORCL\", \"ADBE\", \"CRM\"],\n",
    "#     \"Communication Services\": [\"GOOGL\", \"META\", \"VZ\", \"T\", \"CMCSA\", \"DIS\", \"NFLX\", \"CHTR\", \"TMUS\", \"ATVI\"],\n",
    "#     \"Utilities\": [\"NEE\", \"DUK\", \"SO\", \"D\", \"AEP\", \"EXC\", \"SRE\", \"PEG\", \"ED\", \"XEL\"],\n",
    "#     \"Real Estate\": [\"AMT\", \"PLD\", \"CCI\", \"EQIX\", \"PSA\", \"SPG\", \"WELL\", \"DLR\", \"O\", \"AVB\"]\n",
    "# }\n",
    "\n",
    "# # Function to process the news fetching and writing to file\n",
    "# def process_stock_news(stock_symbol):\n",
    "#     news = fetch_news_for_stock(stock_symbol)\n",
    "    \n",
    "#     # Ensure the 'StockNews' directory exists\n",
    "#     os.makedirs('StockNews', exist_ok=True)\n",
    "    \n",
    "#     # Write to file for each stock symbol\n",
    "#     file_path = os.path.join('StockNews', f'{stock_symbol}.txt')\n",
    "#     with open(file_path, 'w') as file:\n",
    "#         for article in news:\n",
    "#             file.write(f\"Stock: {article['stock']}\\n\")\n",
    "#             file.write(f\"Date: {article['date']}\\n\")\n",
    "#             file.write(f\"Title: {article['title']}\\n\")\n",
    "#             file.write(f\"URL: {article['url']}\\n\")\n",
    "#             file.write(f\"Full Article:\\n{article['full_article']}\\n\")\n",
    "#             file.write(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# # Use ThreadPoolExecutor for concurrent fetching\n",
    "# with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "#     # Iterate through all stocks in the dictionary and submit the fetch task\n",
    "#     for sector, stocks in stock_dict.items():\n",
    "#         executor.map(process_stock_news, stocks)\n",
    "\n",
    "# print(\"News fetching completed for all stocks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff4ec4-a367-45ae-93fc-faf97764664d",
   "metadata": {},
   "source": [
    "**\n",
    "\n",
    "Positive: happy, good, excellent, amazing, great, wonderful, fantastic, outstanding, superb, positive, strong, confident, promising, robust, solid, optimistic, encouraging, favorable, profit, success, growth, high, gain, rise, up, bullish, increase, peak, record-high, outperform, surge, rally, revenue, earnings, dividends, expansion, upgrade, investment, acquisition, innovation, wealth, prosperity, uptrend, rallying, skyrocketing, surge, breakout, rebound, resurgence, breakthrough, recovery, exceeding expectations, soaring, beating estimates, surpassing forecasts, buy, buy-back, top pick, outperform, bullish trend, strong demand, oversubscribed, buy recommendation, institutional buying, accumulation, positive guidance, share repurchase, capital appreciation, strong economy, low unemployment, surplus, GDP growth, high consumer confidence, tax cuts, market optimism, job creation, rising wages, stable inflation, expansionary policy, fiscal stimulus\n",
    "\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256069c2-fe45-46b3-a403-0eedc2c0c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the stock list by sectors\n",
    "# stocks_by_sector = {\n",
    "#     \"Energy\": [\"XOM\", \"CVX\", \"COP\", \"SLB\", \"EOG\", \"OXY\", \"PXD\", \"VLO\", \"MPC\", \"PSX\"],\n",
    "#     \"Materials\": [\"DOW\", \"DD\", \"LIN\", \"APD\", \"SHW\", \"NEM\", \"FCX\", \"ALB\", \"IP\", \"ECL\"],\n",
    "#     \"Industrials\": [\"GE\", \"HON\", \"UNP\", \"BA\", \"MMM\", \"CAT\", \"LMT\", \"RTX\", \"NOC\", \"DE\"],\n",
    "#     \"Consumer Discretionary\": [\"AMZN\", \"TSLA\", \"HD\", \"MCD\", \"NKE\", \"SBUX\", \"BKNG\", \"LOW\", \"TGT\", \"GM\"],\n",
    "#     \"Consumer Staples\": [\"PG\", \"KO\", \"PEP\", \"WMT\", \"COST\", \"PM\", \"MO\", \"MDLZ\", \"CL\", \"KMB\"],\n",
    "#     \"Healthcare\": [\"JNJ\", \"PFE\", \"MRK\", \"ABBV\", \"UNH\", \"AMGN\", \"BMY\", \"GILD\", \"LLY\", \"ANTM\"],\n",
    "#     \"Financials\": [\"JPM\", \"BAC\", \"WFC\", \"C\", \"GS\", \"MS\", \"AXP\", \"USB\", \"PNC\", \"SCHW\"],\n",
    "#     \"Information Technology\": [\"AAPL\", \"MSFT\", \"GOOGL\", \"NVDA\", \"META\", \"INTC\", \"CSCO\", \"ORCL\", \"ADBE\", \"CRM\"],\n",
    "#     \"Communication Services\": [\"GOOGL\", \"META\", \"VZ\", \"T\", \"CMCSA\", \"DIS\", \"NFLX\", \"CHTR\", \"TMUS\", \"ATVI\"],\n",
    "#     \"Utilities\": [\"NEE\", \"DUK\", \"SO\", \"D\", \"AEP\", \"EXC\", \"SRE\", \"PEG\", \"ED\", \"XEL\"],\n",
    "#     \"Real Estate\": [\"AMT\", \"PLD\", \"CCI\", \"EQIX\", \"PSA\", \"SPG\", \"WELL\", \"DLR\", \"O\", \"AVB\"]\n",
    "# }\n",
    "\n",
    "# # Generate URLs for each stock in the list\n",
    "# stock_urls = []\n",
    "# for sector, stocks in stocks_by_sector.items():\n",
    "#     for stock in stocks:\n",
    "#         url = f\"https://finance.yahoo.com/quote/{stock}/news?p={stock}\"\n",
    "#         stock_urls.append(url)\n",
    "\n",
    "# print(stock_urls)  # This will print the list of URLs for all the stocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7ab89f-13eb-4d8f-a56e-d535d6986e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # Directory to store news articles\n",
    "# OUTPUT_DIR = \"News_articles\"\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# # Define the stock list by sectors\n",
    "# stocks_by_sector = {\n",
    "#     \"Energy\": [\"XOM\", \"CVX\", \"COP\", \"SLB\", \"EOG\", \"OXY\", \"PXD\", \"VLO\", \"MPC\", \"PSX\"],\n",
    "#     \"Materials\": [\"DOW\", \"DD\", \"LIN\", \"APD\", \"SHW\", \"NEM\", \"FCX\", \"ALB\", \"IP\", \"ECL\"],\n",
    "#     \"Industrials\": [\"GE\", \"HON\", \"UNP\", \"BA\", \"MMM\", \"CAT\", \"LMT\", \"RTX\", \"NOC\", \"DE\"],\n",
    "#     \"Consumer Discretionary\": [\"AMZN\", \"TSLA\", \"HD\", \"MCD\", \"NKE\", \"SBUX\", \"BKNG\", \"LOW\", \"TGT\", \"GM\"],\n",
    "#     \"Consumer Staples\": [\"PG\", \"KO\", \"PEP\", \"WMT\", \"COST\", \"PM\", \"MO\", \"MDLZ\", \"CL\", \"KMB\"],\n",
    "#     \"Healthcare\": [\"JNJ\", \"PFE\", \"MRK\", \"ABBV\", \"UNH\", \"AMGN\", \"BMY\", \"GILD\", \"LLY\", \"ANTM\"],\n",
    "#     \"Financials\": [\"JPM\", \"BAC\", \"WFC\", \"C\", \"GS\", \"MS\", \"AXP\", \"USB\", \"PNC\", \"SCHW\"],\n",
    "#     \"Information Technology\": [\"AAPL\", \"MSFT\", \"GOOGL\", \"NVDA\", \"META\", \"INTC\", \"CSCO\", \"ORCL\", \"ADBE\", \"CRM\"],\n",
    "#     \"Communication Services\": [\"GOOGL\", \"META\", \"VZ\", \"T\", \"CMCSA\", \"DIS\", \"NFLX\", \"CHTR\", \"TMUS\", \"ATVI\"],\n",
    "#     \"Utilities\": [\"NEE\", \"DUK\", \"SO\", \"D\", \"AEP\", \"EXC\", \"SRE\", \"PEG\", \"ED\", \"XEL\"],\n",
    "#     \"Real Estate\": [\"AMT\", \"PLD\", \"CCI\", \"EQIX\", \"PSA\", \"SPG\", \"WELL\", \"DLR\", \"O\", \"AVB\"]\n",
    "# }\n",
    "\n",
    "# # Function to fetch news articles from Yahoo Finance\n",
    "# def fetch_yahoo_finance_news(stock_symbol):\n",
    "#     url = f\"https://finance.yahoo.com/quote/{stock_symbol}/news?p={stock_symbol}\"\n",
    "    \n",
    "#     # Define headers to avoid being blocked by Yahoo Finance\n",
    "#     headers = {\n",
    "#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "#     }\n",
    "    \n",
    "#     response = requests.get(url, headers=headers)\n",
    "    \n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to fetch news for {stock_symbol}. Status code: {response.status_code}\")\n",
    "#         return []\n",
    "    \n",
    "#     # Parse the HTML content using BeautifulSoup\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "#     # Find the list of news articles\n",
    "#     articles = []\n",
    "#     news_section = soup.find_all('li', class_=\"js-stream-content\")\n",
    "    \n",
    "#     for article in news_section:\n",
    "#         headline = article.find('h3')\n",
    "#         if headline:\n",
    "#             title = headline.text\n",
    "#             link = article.find('a', href=True)['href']\n",
    "#             publication_date = article.find('time')\n",
    "            \n",
    "#             if publication_date:\n",
    "#                 raw_date = publication_date.get('datetime')\n",
    "#                 article_date = datetime.fromisoformat(raw_date)\n",
    "#                 articles.append({\n",
    "#                     \"title\": title,\n",
    "#                     \"url\": f\"https://finance.yahoo.com{link}\",\n",
    "#                     \"date\": article_date\n",
    "#                 })\n",
    "    \n",
    "#     return articles\n",
    "\n",
    "# # Function to filter articles from the last 5 days\n",
    "# def filter_recent_articles(articles, days=5):\n",
    "#     cutoff_date = datetime.now() - timedelta(days=days)\n",
    "#     recent_articles = [article for article in articles if article['date'] >= cutoff_date]\n",
    "    \n",
    "#     return recent_articles\n",
    "\n",
    "# # Function to save articles into text files\n",
    "# def save_articles(stock, articles):\n",
    "#     for article in articles:\n",
    "#         filename = os.path.join(OUTPUT_DIR, f\"{article['date'].strftime('%Y-%m-%d')}_{stock}.txt\")\n",
    "\n",
    "#         with open(filename, \"w\", encoding=\"utf-8\") as f:  # \"w\" to overwrite if needed\n",
    "#             f.write(f\"Stock: {stock}\\n\")\n",
    "#             f.write(f\"Date: {article['date'].strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "#             f.write(f\"Title: {article['title']}\\n\")\n",
    "#             f.write(f\"URL: {article['url']}\\n\")\n",
    "#             f.write(f\"Full Article: [Full article text can be fetched here...]\\n\")\n",
    "\n",
    "#         print(f\"Saved: {filename}\")\n",
    "\n",
    "# # Fetch and save news for each stock\n",
    "# for sector, stocks in stocks_by_sector.items():\n",
    "#     for stock in stocks:\n",
    "#         print(f\"Fetching news for {stock}...\")\n",
    "#         news_articles = fetch_yahoo_finance_news(stock)\n",
    "#         recent_articles = filter_recent_articles(news_articles)\n",
    "#         save_articles(stock, recent_articles)\n",
    "\n",
    "# print(\"News scraping and saving completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c51c815-e019-443d-b56d-2a0da90afec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the Loughran-McDonald Master Dictionary\n",
    "# lm_dict = pd.read_csv(\"C:/Users/91815/OneDrive/Desktop/DESKTOP-D6T8ITB/NAYAN/NAYAN/Desktop/Insight_Fusion_Internship_task/Task_4/my_performence/Loughran-McDonald_MasterDictionary_1993-2023.csv\")\n",
    "\n",
    "# # Extract positive and negative words from the dictionary\n",
    "# pos_words = lm_dict[lm_dict[\"Positive\"] != 0][\"Word\"].str.lower().to_list()\n",
    "# neg_words = lm_dict[lm_dict[\"Negative\"] != 0][\"Word\"].str.lower().to_list()\n",
    "\n",
    "# # Function to perform sentiment analysis based on Loughran-McDonald Dictionary\n",
    "# def perform_sentiment_analysis(article_text):\n",
    "#     words = article_text.lower().split()\n",
    "#     pos_count = sum([1 for word in words if word in pos_words])\n",
    "#     neg_count = sum([1 for word in words if word in neg_words])\n",
    "#     pos_score = pos_count / len(words) if len(words) > 0 else 0\n",
    "#     neg_score = neg_count / len(words) if len(words) > 0 else 0\n",
    "    \n",
    "#     # Sentiment score calculation\n",
    "#     sentiment_score = (pos_score - neg_score) / (pos_score + neg_score) if  (pos_score + neg_score) != 0 else 0 \n",
    "#     return sentiment_score, pos_score, neg_score\n",
    "\n",
    "# # Function to parse date in multiple formats\n",
    "# def parse_date(date_string):\n",
    "#     try:\n",
    "#         return datetime.strptime(date_string, '%Y-%m-%d')\n",
    "#     except ValueError:\n",
    "#         print(f\"Invalid date format: {date_string}\")\n",
    "#         return None\n",
    "\n",
    "# def process_files(input_folder, output_excel):\n",
    "#     data = []\n",
    "\n",
    "#     for filename in os.listdir(input_folder):\n",
    "#         if filename.endswith(\".txt\"):\n",
    "#             with open(os.path.join(input_folder, filename), 'r', encoding='utf-8') as file:\n",
    "#                 lines = file.readlines()\n",
    "\n",
    "#                 stock = date = title = url = full_article = None\n",
    "#                 for line in lines:\n",
    "#                     if line.startswith(\"Stock:\"):\n",
    "#                         stock = line.split(\":\", 1)[1].strip()\n",
    "#                     elif line.startswith(\"Date:\"):\n",
    "#                         date = line.split(\":\", 1)[1].strip()\n",
    "#                     elif line.startswith(\"Title:\"):\n",
    "#                         title = line.split(\":\", 1)[1].strip()\n",
    "#                     elif line.startswith(\"URL:\"):\n",
    "#                         url = line.split(\":\", 1)[1].strip()\n",
    "#                     elif line.startswith(\"Full Article:\"):\n",
    "#                         full_article = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "#                 # Skip entries with no full article\n",
    "#                 if not full_article or full_article.lower() == \"none\":\n",
    "#                     continue\n",
    "\n",
    "#                 # Perform sentiment analysis using Loughran-McDonald Dictionary\n",
    "#                 sentiment_score, pos_score, neg_score = perform_sentiment_analysis(full_article)\n",
    "\n",
    "#                 # Parse the date correctly\n",
    "#                 date = parse_date(date)\n",
    "#                 if not date:\n",
    "#                     print(f\"Skipping file due to invalid date: {filename}\")\n",
    "#                     continue\n",
    "\n",
    "#                 # Ensure stock, date, and other necessary data are available before appending\n",
    "#                 if stock and date and title and url:\n",
    "#                     data.append({\n",
    "#                         'Date': date,\n",
    "#                         'Stock': stock,\n",
    "#                         'Title': title,\n",
    "#                         'URL': url,\n",
    "#                         'Sentiment_score': sentiment_score,\n",
    "#                         'Pos_score': pos_score,\n",
    "#                         'Neg_score': neg_score\n",
    "#                     })\n",
    "\n",
    "#     # Create the DataFrame\n",
    "#     if data:\n",
    "#         df = pd.DataFrame(data)\n",
    "\n",
    "#         # Sort by stock and date (most recent first)\n",
    "#         df.sort_values(by=[\"Stock\", \"Date\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "#         # Calculate weighted average sentiment score based on recency\n",
    "#         final_results = []\n",
    "\n",
    "#         for stock, group in df.groupby('Stock'):\n",
    "#             max_date = group['Date'].max()\n",
    "\n",
    "#             # Calculate time difference in days from the most recent news\n",
    "#             group['Time_Diff_Days'] = (max_date - group['Date']).dt.total_seconds() / (3600 * 24)\n",
    "\n",
    "#             # Exponential decay weights (Smoother decay)\n",
    "#             decay_factor = 0.05  # Adjust this to control how fast older news loses importance\n",
    "#             group['Weight'] = np.exp(-decay_factor * group['Time_Diff_Days'])\n",
    "\n",
    "#             # Normalize weights so they sum to 1\n",
    "#             group['Weight'] = group['Weight'] / group['Weight'].sum()\n",
    "\n",
    "#             # Calculate weighted average sentiment score\n",
    "#             weighted_avg_score = (group['Sentiment_score'] * group['Weight']).sum()\n",
    "\n",
    "#             # Calculate standard deviation to adjust dynamic thresholds\n",
    "#             sentiment_std = group['Sentiment_score'].std()\n",
    "#             sentiment_threshold = sentiment_std  # Using standard deviation to set dynamic thresholds\n",
    "\n",
    "#             # Dynamic final sentiment label based on weighted average score\n",
    "#             if weighted_avg_score >= sentiment_threshold:\n",
    "#                 final_label = 'Positive'\n",
    "#             elif weighted_avg_score <= -sentiment_threshold:\n",
    "#                 final_label = 'Negative'\n",
    "#             else:\n",
    "#                 final_label = 'Neutral'\n",
    "\n",
    "#             for _, row in group.iterrows():\n",
    "#                 final_results.append({\n",
    "#                     'Date': row['Date'],\n",
    "#                     'Stock': stock,\n",
    "#                     'Title': row['Title'],\n",
    "#                     'URL': row['URL'],\n",
    "#                     'Positive_score': row['Pos_score'],\n",
    "#                     'Negative_score': row['Neg_score'],\n",
    "#                     'Neutral_score': 1 - abs(row['Sentiment_score']),\n",
    "#                     'Sentiment_score': row['Sentiment_score'],\n",
    "#                     'Raw_Weight': row['Weight'],  # Added column for raw weight\n",
    "#                     'Weightage': row['Weight'],\n",
    "#                     'Weighted_average_score': weighted_avg_score,\n",
    "#                     'Final_sentiment_label': final_label\n",
    "#                 })\n",
    "\n",
    "#         final_df = pd.DataFrame(final_results)\n",
    "\n",
    "#         # Convert datetime back to string for Excel output\n",
    "#         final_df['Date'] = final_df['Date'].dt.strftime('%d %b %Y %H:%M')\n",
    "\n",
    "#         # Write the output to an Excel file\n",
    "#         final_df.to_excel(output_excel, index=False)\n",
    "\n",
    "#     else:\n",
    "#         print(\"No valid data to process.\")\n",
    "\n",
    "# # Folder containing text files\n",
    "# input_folder = 'news_articles'  \n",
    "# output_excel = 'filtered_output1.xlsx'\n",
    "\n",
    "# process_files(input_folder, output_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baca0e24-32f8-4a94-bc79-c4d9d5e67393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import random\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urlencode\n",
    "# from datetime import datetime, timedelta\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# # Configuration\n",
    "# stocks = [\"XOM\", \"CVX\", \"COP\", \"SLB\", \"EOG\", \"OXY\", \"PXD\", \"VLO\", \"MPC\", \"PSX\"]\n",
    "# OUTPUT_DIR = \"News_articles\"\n",
    "# OUTPUT_EXCEL = \"stock_sentiment_output_final.xlsx\"\n",
    "# HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "# DATE_RANGES = [\"1d\", \"2d\", \"3d\", \"4d\", \"5d\"]\n",
    "# DELAY_ENABLED = True  # Set False for faster testing\n",
    "\n",
    "# # Sentiment Analyzer\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # Create output folder\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# def fetch_articles(stock, date_range):\n",
    "#     \"\"\"Fetch news articles for a specific stock and date range.\"\"\"\n",
    "#     params = {\n",
    "#         \"q\": f\"{stock} stock news\",\n",
    "#         \"tbm\": \"nws\",\n",
    "#         \"tbs\": f\"qdr:{date_range}\",\n",
    "#         \"hl\": \"en\",\n",
    "#         \"gl\": \"us\"\n",
    "#     }\n",
    "#     url = f\"https://www.google.com/search?{urlencode(params)}\"\n",
    "#     response = requests.get(url, headers=HEADERS)\n",
    "#     articles = []\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         for article in soup.select(\".SoaBEf\"):\n",
    "#             title = article.select_one(\".nDgy9d\").text if article.select_one(\".nDgy9d\") else \"No Title\"\n",
    "#             summary = article.select_one(\".Y3v8qd\").text if article.select_one(\".Y3v8qd\") else \"No Summary\"\n",
    "#             url = article.select_one(\"a\")[\"href\"] if article.select_one(\"a\") else \"No URL\"\n",
    "#             raw_date = article.select_one(\".OSrXXb\").text if article.select_one(\".OSrXXb\") else \"Unknown Date\"\n",
    "#             formatted_date = convert_relative_date(raw_date)\n",
    "\n",
    "#             full_text = fetch_full_article(url)\n",
    "\n",
    "#             articles.append({\n",
    "#                 \"title\": title,\n",
    "#                 \"summary\": summary,\n",
    "#                 \"url\": url,\n",
    "#                 \"date\": formatted_date,\n",
    "#                 \"full_article\": full_text\n",
    "#             })\n",
    "#     else:\n",
    "#         print(f\"[ERROR] Failed to fetch {stock} for {date_range}\")\n",
    "\n",
    "#     return articles\n",
    "\n",
    "\n",
    "# def fetch_full_article(url):\n",
    "#     \"\"\"Fetch full text of an article.\"\"\"\n",
    "#     try:\n",
    "#         response = requests.get(url, headers=HEADERS, timeout=5)\n",
    "#         if response.status_code == 200:\n",
    "#             soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#             paragraphs = soup.find_all('p')\n",
    "#             return \"\\n\".join(p.text for p in paragraphs) or \"No Content\"\n",
    "#     except:\n",
    "#         pass\n",
    "#     return \"Failed to fetch article.\"\n",
    "\n",
    "\n",
    "# def convert_relative_date(relative_date):\n",
    "#     \"\"\"Convert relative date strings like '2 days ago' to absolute dates.\"\"\"\n",
    "#     now = datetime.today()\n",
    "#     date_parts = relative_date.split()\n",
    "#     if not date_parts or not date_parts[0].isdigit():\n",
    "#         return now.strftime(\"%Y-%m-%d\")\n",
    "#     num = int(date_parts[0])\n",
    "#     if \"hour\" in relative_date:\n",
    "#         article_date = now - timedelta(hours=num)\n",
    "#     elif \"day\" in relative_date:\n",
    "#         article_date = now - timedelta(days=num)\n",
    "#     elif \"week\" in relative_date:\n",
    "#         article_date = now - timedelta(weeks=num)\n",
    "#     else:\n",
    "#         article_date = now\n",
    "#     return article_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# def perform_sentiment_analysis(text):\n",
    "#     \"\"\"Perform sentiment analysis using VADER.\"\"\"\n",
    "#     scores = analyzer.polarity_scores(text)\n",
    "#     return scores\n",
    "\n",
    "\n",
    "# def assign_weights(group):\n",
    "#     \"\"\"Assign exponential decay weights to articles.\"\"\"\n",
    "#     max_date = group['date'].max()\n",
    "#     group['days_diff'] = (max_date - group['date']).dt.days\n",
    "#     group['weight'] = np.exp(-0.5 * group['days_diff'])\n",
    "#     group['weight'] /= group['weight'].sum()\n",
    "#     return group\n",
    "\n",
    "\n",
    "# def fetch_and_analyze(stock):\n",
    "#     \"\"\"Fetch and analyze news for a single stock.\"\"\"\n",
    "#     print(f\"\\n[START] Processing {stock}\")\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     all_articles = []\n",
    "#     with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "#         future_to_range = {executor.submit(fetch_articles, stock, dr): dr for dr in DATE_RANGES}\n",
    "\n",
    "#         for future in as_completed(future_to_range):\n",
    "#             try:\n",
    "#                 articles = future.result()\n",
    "#                 all_articles.extend(articles)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[ERROR] {stock}: {e}\")\n",
    "\n",
    "#             if DELAY_ENABLED:\n",
    "#                 time.sleep(random.uniform(2, 5))\n",
    "\n",
    "#     if not all_articles:\n",
    "#         return []\n",
    "\n",
    "#     df = pd.DataFrame(all_articles)\n",
    "#     df['date'] = pd.to_datetime(df['date'])\n",
    "#     sentiment_scores = df['full_article'].apply(perform_sentiment_analysis)\n",
    "#     df['sentiment_score'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "#     df['positive_score'] = sentiment_scores.apply(lambda x: x['pos'])\n",
    "#     df['negative_score'] = sentiment_scores.apply(lambda x: x['neg'])\n",
    "\n",
    "#     df = df.sort_values(by='date', ascending=False).head(5)\n",
    "#     df = assign_weights(df)\n",
    "\n",
    "#     weighted_avg_score = (df['sentiment_score'] * df['weight']).sum()\n",
    "#     sentiment_std = df['sentiment_score'].std()\n",
    "\n",
    "#     # Final Sentiment Labels\n",
    "#     if pd.isna(sentiment_std):\n",
    "#         sentiment_std = 0\n",
    "\n",
    "#     final_label_std = 'Positive' if weighted_avg_score >= sentiment_std else 'Negative' if weighted_avg_score <= -sentiment_std else 'Neutral'\n",
    "#     final_label_threshold = 'Positive' if weighted_avg_score > 0.4 else 'Negative' if weighted_avg_score < -0.4 else 'Neutral'\n",
    "\n",
    "#     records = []\n",
    "#     for _, row in df.iterrows():\n",
    "#         records.append({\n",
    "#             'Date': row['date'].strftime('%d %b %Y %H:%M'),\n",
    "#             'Stock': stock,\n",
    "#             'Title': row['title'],\n",
    "#             'URL': row['url'],\n",
    "#             'Positive_score': row['positive_score'],\n",
    "#             'Negative_score': row['negative_score'],\n",
    "#             'Sentiment_score': row['sentiment_score'],\n",
    "#             'Weightage': row['weight'],\n",
    "#             'Weighted_average_score': weighted_avg_score,\n",
    "#             'Final_sentiment_label_on_std': final_label_std,\n",
    "#             'Final_sentiment_label_on_threshold_value': final_label_threshold\n",
    "#         })\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     print(f\"[COMPLETED] {stock} in {end_time - start_time:.2f}s\")\n",
    "#     return records\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     all_results = []\n",
    "#     for stock in stocks:\n",
    "#         stock_results = fetch_and_analyze(stock)\n",
    "#         if stock_results:\n",
    "#             all_results.extend(stock_results)\n",
    "\n",
    "#         if DELAY_ENABLED:\n",
    "#             time.sleep(random.uniform(5, 10))\n",
    "\n",
    "#     if all_results:\n",
    "#         df = pd.DataFrame(all_results)\n",
    "#         df.to_excel(OUTPUT_EXCEL, index=False)\n",
    "#         print(f\"\\n[FINISHED] Results saved to {OUTPUT_EXCEL}\")\n",
    "#     else:\n",
    "#         print(\"\\n[INFO] No data to save.\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2c1964f-c9a7-471f-a495-b330f507bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876225c5-5c62-4668-b8cb-6da9999316ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import random\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urlencode\n",
    "# from datetime import datetime, timedelta\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# # Configuration\n",
    "# stocks = [\"XOM\", \"CVX\"]\n",
    "# OUTPUT_DIR = \"News_articles\"\n",
    "# OUTPUT_EXCEL = \"stock_sentiment_output_final.xlsx\"\n",
    "# HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "# DATE_RANGES = [\"1d\", \"2d\", \"3d\", \"4d\", \"5d\"]\n",
    "# DELAY_ENABLED = True  # Set False for faster testing\n",
    "\n",
    "# # Sentiment Analyzer\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # Create output folder\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# def fetch_articles(stock, date_range):\n",
    "#     \"\"\"Fetch news articles for a specific stock and date range.\"\"\"\n",
    "#     params = {\n",
    "#         \"q\": f\"{stock} stock news\",\n",
    "#         \"tbm\": \"nws\",\n",
    "#         \"tbs\": f\"qdr:{date_range}\",\n",
    "#         \"hl\": \"en\",\n",
    "#         \"gl\": \"us\"\n",
    "#     }\n",
    "#     url = f\"https://www.google.com/search?{urlencode(params)}\"\n",
    "#     response = requests.get(url, headers=HEADERS)\n",
    "#     articles = []\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         seen_urls = set()  # To track seen article URLs\n",
    "#         for article in soup.select(\".SoaBEf\"):\n",
    "#             title = article.select_one(\".nDgy9d\").text if article.select_one(\".nDgy9d\") else \"No Title\"\n",
    "#             summary = article.select_one(\".Y3v8qd\").text if article.select_one(\".Y3v8qd\") else \"No Summary\"\n",
    "#             url = article.select_one(\"a\")[\"href\"] if article.select_one(\"a\") else \"No URL\"\n",
    "#             raw_date = article.select_one(\".OSrXXb\").text if article.select_one(\".OSrXXb\") else \"Unknown Date\"\n",
    "#             formatted_date = convert_relative_date(raw_date)\n",
    "\n",
    "#             if url not in seen_urls:  # Avoid duplicates based on URL\n",
    "#                 seen_urls.add(url)\n",
    "#                 full_text = fetch_full_article(url)\n",
    "\n",
    "#                 articles.append({\n",
    "#                     \"title\": title,\n",
    "#                     \"summary\": summary,\n",
    "#                     \"url\": url,\n",
    "#                     \"date\": formatted_date,\n",
    "#                     \"full_article\": full_text\n",
    "#                 })\n",
    "\n",
    "#     else:\n",
    "#         print(f\"[ERROR] Failed to fetch {stock} for {date_range}\")\n",
    "\n",
    "#     return articles\n",
    "\n",
    "# def fetch_full_article(url):\n",
    "#     \"\"\"Fetch full text of an article.\"\"\"\n",
    "#     try:\n",
    "#         response = requests.get(url, headers=HEADERS, timeout=5)\n",
    "#         if response.status_code == 200:\n",
    "#             soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#             paragraphs = soup.find_all('p')\n",
    "#             return \"\\n\".join(p.text for p in paragraphs) or \"No Content\"\n",
    "#     except:\n",
    "#         pass\n",
    "#     return \"Failed to fetch article.\"\n",
    "\n",
    "# def convert_relative_date(relative_date):\n",
    "#     \"\"\"Convert relative date strings like '2 days ago' to absolute dates.\"\"\"\n",
    "#     now = datetime.today()\n",
    "#     date_parts = relative_date.split()\n",
    "#     if not date_parts or not date_parts[0].isdigit():\n",
    "#         return now.strftime(\"%Y-%m-%d\")\n",
    "#     num = int(date_parts[0])\n",
    "#     if \"hour\" in relative_date:\n",
    "#         article_date = now - timedelta(hours=num)\n",
    "#     elif \"day\" in relative_date:\n",
    "#         article_date = now - timedelta(days=num)\n",
    "#     elif \"week\" in relative_date:\n",
    "#         article_date = now - timedelta(weeks=num)\n",
    "#     else:\n",
    "#         article_date = now\n",
    "#     return article_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# def perform_sentiment_analysis(text):\n",
    "#     \"\"\"Perform sentiment analysis using VADER.\"\"\"\n",
    "#     scores = analyzer.polarity_scores(text)\n",
    "#     return scores\n",
    "\n",
    "# def assign_weights(group):\n",
    "#     \"\"\"Assign exponential decay weights to articles.\"\"\"\n",
    "#     max_date = group['date'].max()\n",
    "#     group['days_diff'] = (max_date - group['date']).dt.days\n",
    "#     group['weight'] = np.exp(-0.5 * group['days_diff'])\n",
    "#     group['weight'] /= group['weight'].sum()\n",
    "#     return group\n",
    "\n",
    "# def fetch_and_analyze(stock):\n",
    "#     \"\"\"Fetch and analyze news for a single stock.\"\"\"\n",
    "#     print(f\"\\n[START] Processing {stock}\")\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     all_articles = []\n",
    "#     with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "#         future_to_range = {executor.submit(fetch_articles, stock, dr): dr for dr in DATE_RANGES}\n",
    "\n",
    "#         for future in as_completed(future_to_range):\n",
    "#             try:\n",
    "#                 articles = future.result()\n",
    "#                 all_articles.extend(articles)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[ERROR] {stock}: {e}\")\n",
    "\n",
    "#             if DELAY_ENABLED:\n",
    "#                 time.sleep(random.uniform(2, 5))\n",
    "\n",
    "#     if not all_articles:\n",
    "#         return []\n",
    "\n",
    "#     df = pd.DataFrame(all_articles)\n",
    "#     df['date'] = pd.to_datetime(df['date'])\n",
    "#     sentiment_scores = df['full_article'].apply(perform_sentiment_analysis)\n",
    "#     df['sentiment_score'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "#     df['positive_score'] = sentiment_scores.apply(lambda x: x['pos'])\n",
    "#     df['negative_score'] = sentiment_scores.apply(lambda x: x['neg'])\n",
    "\n",
    "#     # Filter out articles that are older than the last 5 days\n",
    "#     five_days_ago = datetime.today() - timedelta(days=5)\n",
    "#     df = df[df['date'] >= five_days_ago]\n",
    "\n",
    "#     df = df.sort_values(by='date', ascending=False).head(5)\n",
    "#     df = assign_weights(df)\n",
    "\n",
    "#     weighted_avg_score = (df['sentiment_score'] * df['weight']).sum()\n",
    "#     sentiment_std = df['sentiment_score'].std()\n",
    "\n",
    "#     # Final Sentiment Labels\n",
    "#     if pd.isna(sentiment_std):\n",
    "#         sentiment_std = 0\n",
    "\n",
    "#     final_label_std = 'Positive' if weighted_avg_score >= sentiment_std else 'Negative' if weighted_avg_score <= -sentiment_std else 'Neutral'\n",
    "#     final_label_threshold = 'Positive' if weighted_avg_score > 0.4 else 'Negative' if weighted_avg_score < -0.4 else 'Neutral'\n",
    "\n",
    "#     records = []\n",
    "#     for _, row in df.iterrows():\n",
    "#         records.append({\n",
    "#             'Date': row['date'].strftime('%d %b %Y %H:%M'),\n",
    "#             'Stock': stock,\n",
    "#             'Title': row['title'],\n",
    "#             'URL': row['url'],\n",
    "#             'Positive_score': row['positive_score'],\n",
    "#             'Negative_score': row['negative_score'],\n",
    "#             'Sentiment_score': row['sentiment_score'],\n",
    "#             'Weightage': row['weight'],\n",
    "#             'Weighted_average_score': weighted_avg_score,\n",
    "#             'Final_sentiment_label_on_std': final_label_std,\n",
    "#             'Final_sentiment_label_on_threshold_value': final_label_threshold\n",
    "#         })\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     print(f\"[COMPLETED] {stock} in {end_time - start_time:.2f}s\")\n",
    "#     return records\n",
    "\n",
    "# def main():\n",
    "#     all_results = []\n",
    "#     for stock in stocks:\n",
    "#         stock_results = fetch_and_analyze(stock)\n",
    "#         if stock_results:\n",
    "#             all_results.extend(stock_results)\n",
    "\n",
    "#         if DELAY_ENABLED:\n",
    "#             time.sleep(random.uniform(5, 10))\n",
    "\n",
    "#     if all_results:\n",
    "#         df = pd.DataFrame(all_results)\n",
    "#         df.to_excel(OUTPUT_EXCEL, index=False)\n",
    "#         print(f\"\\n[FINISHED] Results saved to {OUTPUT_EXCEL}\")\n",
    "#     else:\n",
    "#         print(\"\\n[INFO] No data to save.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f90b013-1cdb-4079-8b2b-af7c9e38576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import random\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urlencode\n",
    "# from datetime import datetime, timedelta\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# # Configuration\n",
    "# stocks = [\"XOM\", \"CVX\"]\n",
    "# OUTPUT_DIR = \"News_articles\"\n",
    "# OUTPUT_EXCEL = \"stock_sentiment_output_final.xlsx\"\n",
    "# HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "# DATE_RANGES = [\"1d\", \"2d\", \"3d\", \"4d\", \"5d\"]\n",
    "# DELAY_ENABLED = True  # Set False for faster testing\n",
    "# MAX_RETRIES = 3\n",
    "\n",
    "# # Sentiment Analyzer\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # Create output folder\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# def fetch_articles(stock, date_range):\n",
    "#     \"\"\"Fetch news articles for a specific stock and date range.\"\"\"\n",
    "#     params = {\n",
    "#         \"q\": f\"{stock} stock news\",\n",
    "#         \"tbm\": \"nws\",\n",
    "#         \"tbs\": f\"qdr:{date_range}\",\n",
    "#         \"hl\": \"en\",\n",
    "#         \"gl\": \"us\"\n",
    "#     }\n",
    "#     url = f\"https://www.google.com/search?{urlencode(params)}\"\n",
    "#     response = requests.get(url, headers=HEADERS)\n",
    "#     articles = []\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         seen_urls = set()\n",
    "#         for article in soup.select(\".SoaBEf\"):\n",
    "#             title = article.select_one(\".nDgy9d\").text if article.select_one(\".nDgy9d\") else \"No Title\"\n",
    "#             summary = article.select_one(\".Y3v8qd\").text if article.select_one(\".Y3v8qd\") else \"No Summary\"\n",
    "#             url = article.select_one(\"a\")[\"href\"] if article.select_one(\"a\") else \"No URL\"\n",
    "#             raw_date = article.select_one(\".OSrXXb\").text if article.select_one(\".OSrXXb\") else \"Unknown Date\"\n",
    "#             formatted_date = convert_relative_date(raw_date)\n",
    "\n",
    "#             if url not in seen_urls:\n",
    "#                 seen_urls.add(url)\n",
    "#                 articles.append({\n",
    "#                     \"title\": title,\n",
    "#                     \"summary\": summary,\n",
    "#                     \"url\": url,\n",
    "#                     \"date\": formatted_date\n",
    "#                 })\n",
    "\n",
    "#     else:\n",
    "#         print(f\"[ERROR] Failed to fetch {stock} for {date_range}\")\n",
    "\n",
    "#     return articles\n",
    "\n",
    "\n",
    "# def fetch_full_article(url):\n",
    "#     \"\"\"Fetch full text of an article with retries.\"\"\"\n",
    "#     for _ in range(MAX_RETRIES):\n",
    "#         try:\n",
    "#             response = requests.get(url, headers=HEADERS, timeout=5)\n",
    "#             if response.status_code == 200:\n",
    "#                 soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#                 paragraphs = soup.find_all('p')\n",
    "#                 return \"\\n\".join(p.text for p in paragraphs) or \"No Content\"\n",
    "#         except requests.RequestException as e:\n",
    "#             print(f\"[ERROR] Failed to fetch article {url}: {e}\")\n",
    "#             time.sleep(1)\n",
    "\n",
    "#     return \"Failed to fetch article.\"\n",
    "\n",
    "\n",
    "# def convert_relative_date(relative_date):\n",
    "#     \"\"\"Convert relative date strings like '2 days ago' to absolute dates.\"\"\"\n",
    "#     now = datetime.today()\n",
    "#     date_parts = relative_date.split()\n",
    "#     if not date_parts or not date_parts[0].isdigit():\n",
    "#         return now.strftime(\"%Y-%m-%d\")\n",
    "#     num = int(date_parts[0])\n",
    "#     if \"hour\" in relative_date:\n",
    "#         article_date = now - timedelta(hours=num)\n",
    "#     elif \"day\" in relative_date:\n",
    "#         article_date = now - timedelta(days=num)\n",
    "#     elif \"week\" in relative_date:\n",
    "#         article_date = now - timedelta(weeks=num)\n",
    "#     else:\n",
    "#         article_date = now\n",
    "#     return article_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# def perform_sentiment_analysis(text):\n",
    "#     \"\"\"Perform sentiment analysis using VADER.\"\"\"\n",
    "#     scores = analyzer.polarity_scores(text)\n",
    "#     return scores\n",
    "\n",
    "\n",
    "# def assign_weights(group):\n",
    "#     \"\"\"Assign exponential decay weights to articles.\"\"\"\n",
    "#     max_date = group['date'].max()\n",
    "#     group['days_diff'] = (max_date - group['date']).dt.days\n",
    "#     group['weight'] = np.exp(-0.5 * group['days_diff'])\n",
    "#     group['weight'] /= group['weight'].sum()\n",
    "#     return group\n",
    "\n",
    "\n",
    "# def fetch_and_analyze(stock):\n",
    "#     \"\"\"Fetch and analyze news for a single stock.\"\"\"\n",
    "#     print(f\"\\n[START] Processing {stock}\")\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     all_articles = []\n",
    "#     with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "#         future_to_range = {executor.submit(fetch_articles, stock, dr): dr for dr in DATE_RANGES}\n",
    "#         for future in as_completed(future_to_range):\n",
    "#             try:\n",
    "#                 articles = future.result()\n",
    "#                 all_articles.extend(articles)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[ERROR] {stock}: {e}\")\n",
    "\n",
    "#     # Remove duplicate articles based on URL and Title\n",
    "#     df = pd.DataFrame(all_articles).drop_duplicates(subset=['url', 'title'])\n",
    "\n",
    "#     # Fetch full articles\n",
    "#     df['full_article'] = df['url'].apply(fetch_full_article)\n",
    "\n",
    "#     # Convert dates to datetime\n",
    "#     df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "#     # Perform Sentiment Analysis\n",
    "#     sentiment_scores = df['full_article'].apply(perform_sentiment_analysis)\n",
    "#     df['sentiment_score'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "#     df['positive_score'] = sentiment_scores.apply(lambda x: x['pos'])\n",
    "#     df['negative_score'] = sentiment_scores.apply(lambda x: x['neg'])\n",
    "\n",
    "#     # Filter out articles older than the last 5 days\n",
    "#     five_days_ago = datetime.today() - timedelta(days=5)\n",
    "#     df = df[df['date'] >= five_days_ago]\n",
    "\n",
    "#     # Pick top 5 most recent articles\n",
    "#     df = df.sort_values(by='date', ascending=False).head(5)\n",
    "#     if df.empty:\n",
    "#         return []\n",
    "\n",
    "#     # Assign Weights\n",
    "#     df = assign_weights(df)\n",
    "\n",
    "#     # Calculate Weighted Sentiment Score\n",
    "#     weighted_avg_score = (df['sentiment_score'] * df['weight']).sum()\n",
    "#     sentiment_std = df['sentiment_score'].std() or 0\n",
    "\n",
    "#     final_label_std = 'Positive' if weighted_avg_score >= sentiment_std else 'Negative' if weighted_avg_score <= -sentiment_std else 'Neutral'\n",
    "#     final_label_threshold = 'Positive' if weighted_avg_score > 0.4 else 'Negative' if weighted_avg_score < -0.4 else 'Neutral'\n",
    "\n",
    "#     records = []\n",
    "#     for _, row in df.iterrows():\n",
    "#         records.append({\n",
    "#             'Date': row['date'].strftime('%d %b %Y %H:%M'),\n",
    "#             'Stock': stock,\n",
    "#             'Title': row['title'],\n",
    "#             'URL': row['url'],\n",
    "#             'Positive_score': row['positive_score'],\n",
    "#             'Negative_score': row['negative_score'],\n",
    "#             'Sentiment_score': row['sentiment_score'],\n",
    "#             'Weightage': row['weight'],\n",
    "#             'Weighted_average_score': weighted_avg_score,\n",
    "#             'Final_sentiment_label_on_std': final_label_std,\n",
    "#             'Final_sentiment_label_on_threshold_value': final_label_threshold\n",
    "#         })\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     print(f\"[COMPLETED] {stock} in {end_time - start_time:.2f}s\")\n",
    "#     return records\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     all_results = []\n",
    "#     for stock in stocks:\n",
    "#         stock_results = fetch_and_analyze(stock)\n",
    "#         if stock_results:\n",
    "#             all_results.extend(stock_results)\n",
    "\n",
    "#     if all_results:\n",
    "#         df = pd.DataFrame(all_results)\n",
    "#         df.to_excel(OUTPUT_EXCEL, index=False)\n",
    "#         print(f\"\\n[FINISHED] Results saved to {OUTPUT_EXCEL}\")\n",
    "#     else:\n",
    "#         print(\"\\n[INFO] No data to save.\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "194655d1-45c2-4c86-ace3-2f4a536e36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import random\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urlencode, urlparse, parse_qs\n",
    "# from datetime import datetime, timedelta\n",
    "# from newspaper import Article\n",
    "\n",
    "# # Stocks to scrape news for\n",
    "# stocks = [\"INFY\", \"XOM\", \"CVX\", \"COP\"]\n",
    "\n",
    "# GOOGLE_NEWS_URL = \"https://www.google.com/search\"\n",
    "# HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "# OUTPUT_DIR = \"news_articles\"\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# MAX_ARTICLES = 5  # Max valid articles per stock\n",
    "\n",
    "# def fetch_full_article(url):\n",
    "#     try:\n",
    "#         article = Article(url)\n",
    "#         article.download()\n",
    "#         article.parse()\n",
    "#         return article.text\n",
    "#     except Exception as e:\n",
    "#         return \"\"\n",
    "\n",
    "# def clean_google_url(google_url):\n",
    "#     parsed_url = urlparse(google_url)\n",
    "#     query_params = parse_qs(parsed_url.query)\n",
    "#     return query_params.get(\"q\", [google_url])[0]\n",
    "\n",
    "# def convert_relative_date(relative_date):\n",
    "#     now = datetime.today()\n",
    "#     if 'hour' in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(hours=num)\n",
    "#     elif 'day' in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(days=num)\n",
    "#     elif 'week' in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(weeks=num)\n",
    "#     elif 'minute' in relative_date:\n",
    "#         article_date = now\n",
    "#     else:\n",
    "#         article_date = now\n",
    "#     return article_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# def scrape_news(stock):\n",
    "#     print(f\"Fetching news for: {stock}...\")\n",
    "#     valid_articles = []\n",
    "#     params = {\"q\": f\"{stock} stock news\", \"tbm\": \"nws\", \"hl\": \"en\", \"gl\": \"us\"}\n",
    "#     url = f\"{GOOGLE_NEWS_URL}?{urlencode(params)}\"\n",
    "#     response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to fetch news for {stock}\")\n",
    "#         return []\n",
    "\n",
    "#     soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#     for article in soup.select(\".SoaBEf\"):\n",
    "#         if len(valid_articles) >= MAX_ARTICLES:\n",
    "#             break\n",
    "\n",
    "#         title_element = article.select_one(\".nDgy9d\")\n",
    "#         url_element = article.select_one(\"a\")\n",
    "#         date_element = article.select_one(\".OSrXXb\")\n",
    "\n",
    "#         title = title_element.text.strip() if title_element else \"No Title\"\n",
    "#         url = url_element[\"href\"] if url_element else \"\"\n",
    "#         raw_date = date_element.text.strip() if date_element else \"Unknown Date\"\n",
    "\n",
    "#         if url.startswith(\"/url?\"):\n",
    "#             url = clean_google_url(url)\n",
    "\n",
    "#         full_article = fetch_full_article(url)\n",
    "#         if not full_article.strip():\n",
    "#             continue  # Skip if article is empty\n",
    "\n",
    "#         valid_articles.append({\n",
    "#             \"title\": title,\n",
    "#             \"url\": url,\n",
    "#             \"date\": convert_relative_date(raw_date),\n",
    "#             \"full_article\": full_article\n",
    "#         })\n",
    "\n",
    "#         if len(valid_articles) >= MAX_ARTICLES:\n",
    "#             break\n",
    "\n",
    "#     return valid_articles\n",
    "\n",
    "# def save_articles(stock, articles):\n",
    "#     for article in articles:\n",
    "#         filename = os.path.join(OUTPUT_DIR, f\"{article['date']}_{stock}.txt\")\n",
    "#         with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(f\"Stock: {stock}\\n\")\n",
    "#             f.write(f\"Date: {article['date']}\\n\")\n",
    "#             f.write(f\"Title: {article['title']}\\n\")\n",
    "#             f.write(f\"URL: {article['url']}\\n\")\n",
    "#             f.write(f\"Full Article:\\n{article['full_article']}\\n\")\n",
    "#         print(f\"Saved: {filename}\")\n",
    "\n",
    "# for stock in stocks:\n",
    "#     articles = scrape_news(stock)\n",
    "#     if articles:\n",
    "#         save_articles(stock, articles)\n",
    "#     else:\n",
    "#         print(f\"No valid articles found for {stock}.\")\n",
    "\n",
    "#     time.sleep(random.uniform(5, 10))  # Sleep to avoid rate limits\n",
    "\n",
    "# print(\"News scraping completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de05e332-1afe-4c17-8a7c-bd1776af8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import random\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urlencode, urlparse, parse_qs\n",
    "# from datetime import datetime, timedelta\n",
    "# from newspaper import Article\n",
    "\n",
    "# # Stocks to scrape news for\n",
    "# stocks = [\"INFY\", \"XOM\", \"CVX\", \"COP\"]\n",
    "\n",
    "# GOOGLE_NEWS_URL = \"https://www.google.com/search\"\n",
    "# HEADERS = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "# }\n",
    "# OUTPUT_DIR = \"news_articles\"\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# FETCH_LIMIT = 15  # Fetch more to ensure we get at least 5 valid articles\n",
    "# MAX_VALID_ARTICLES = 5  # We only need 5 valid articles per stock\n",
    "\n",
    "\n",
    "# def fetch_full_article(url):\n",
    "#     try:\n",
    "#         article = Article(url)\n",
    "#         article.download()\n",
    "#         article.parse()\n",
    "#         return article.text\n",
    "#     except Exception as e:\n",
    "#         return \"\"\n",
    "\n",
    "\n",
    "# def clean_google_url(google_url):\n",
    "#     parsed_url = urlparse(google_url)\n",
    "#     query_params = parse_qs(parsed_url.query)\n",
    "#     return query_params.get(\"q\", [google_url])[0]\n",
    "\n",
    "\n",
    "# def convert_relative_date(relative_date):\n",
    "#     now = datetime.today()\n",
    "#     if \"hour\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(hours=num)\n",
    "#     elif \"day\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(days=num)\n",
    "#     elif \"week\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(weeks=num)\n",
    "#     elif \"minute\" in relative_date:\n",
    "#         article_date = now\n",
    "#     else:\n",
    "#         article_date = now\n",
    "#     return article_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# def scrape_news(stock):\n",
    "#     print(f\"Fetching news for: {stock}...\")\n",
    "#     valid_articles = []\n",
    "#     params = {\"q\": f\"{stock} stock news\", \"tbm\": \"nws\", \"hl\": \"en\", \"gl\": \"us\"}\n",
    "#     url = f\"{GOOGLE_NEWS_URL}?{urlencode(params)}\"\n",
    "#     response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to fetch news for {stock}\")\n",
    "#         return []\n",
    "\n",
    "#     soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#     articles_fetched = 0\n",
    "\n",
    "#     for article in soup.select(\".SoaBEf\"):\n",
    "#         if articles_fetched >= FETCH_LIMIT:\n",
    "#             break\n",
    "\n",
    "#         title_element = article.select_one(\".nDgy9d\")\n",
    "#         url_element = article.select_one(\"a\")\n",
    "#         date_element = article.select_one(\".OSrXXb\")\n",
    "\n",
    "#         title = title_element.text.strip() if title_element else \"No Title\"\n",
    "#         url = url_element[\"href\"] if url_element else \"\"\n",
    "#         raw_date = date_element.text.strip() if date_element else \"Unknown Date\"\n",
    "\n",
    "#         if url.startswith(\"/url?\"):\n",
    "#             url = clean_google_url(url)\n",
    "\n",
    "#         full_article = fetch_full_article(url)\n",
    "#         if not full_article.strip():\n",
    "#             continue  # Skip if article is empty\n",
    "\n",
    "#         valid_articles.append({\n",
    "#             \"title\": title,\n",
    "#             \"url\": url,\n",
    "#             \"date\": convert_relative_date(raw_date),\n",
    "#             \"full_article\": full_article,\n",
    "#         })\n",
    "\n",
    "#         articles_fetched += 1\n",
    "\n",
    "#         if len(valid_articles) >= MAX_VALID_ARTICLES:\n",
    "#             break\n",
    "\n",
    "#     return valid_articles\n",
    "\n",
    "\n",
    "# def save_articles(stock, articles):\n",
    "#     date_counter = {}\n",
    "\n",
    "#     for i, article in enumerate(articles):\n",
    "#         base_filename = f\"{article['date']}_{stock}\"\n",
    "\n",
    "#         # Handle duplicate dates by appending a counter\n",
    "#         if article['date'] not in date_counter:\n",
    "#             date_counter[article['date']] = 1\n",
    "#         else:\n",
    "#             date_counter[article['date']] += 1\n",
    "\n",
    "#         if date_counter[article['date']] > 1:\n",
    "#             filename = f\"{base_filename}_{date_counter[article['date']]}.txt\"\n",
    "#         else:\n",
    "#             filename = f\"{base_filename}.txt\"\n",
    "\n",
    "#         filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "#         with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(f\"Stock: {stock}\\n\")\n",
    "#             f.write(f\"Date: {article['date']}\\n\")\n",
    "#             f.write(f\"Title: {article['title']}\\n\")\n",
    "#             f.write(f\"URL: {article['url']}\\n\")\n",
    "#             f.write(f\"Full Article:\\n{article['full_article']}\\n\")\n",
    "\n",
    "#         print(f\"Saved: {filepath}\")\n",
    "\n",
    "\n",
    "# for stock in stocks:\n",
    "#     articles = scrape_news(stock)\n",
    "#     if articles:\n",
    "#         save_articles(stock, articles[:MAX_VALID_ARTICLES])  # Save exactly 5 or less valid ones\n",
    "#     else:\n",
    "#         print(f\"No valid articles found for {stock}.\")\n",
    "\n",
    "#     time.sleep(random.uniform(5, 10))  # Sleep to avoid rate limits\n",
    "\n",
    "# print(\"News scraping completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a8089a0-c345-493a-bad7-ea755603991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# from datetime import datetime\n",
    "\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# def perform_sentiment_analysis(article_text):\n",
    "#     vader_scores = analyzer.polarity_scores(article_text)\n",
    "#     return vader_scores['compound']\n",
    "\n",
    "# def parse_date(date_string):\n",
    "#     formats = ['%Y-%m-%d', '%d-%m-%Y', '%d/%m/%Y']\n",
    "#     for fmt in formats:\n",
    "#         try:\n",
    "#             return datetime.strptime(date_string, fmt)\n",
    "#         except ValueError:\n",
    "#             continue\n",
    "#     print(f\"Invalid date format: {date_string}\")\n",
    "#     return None\n",
    "\n",
    "# def assign_dynamic_weights(group):\n",
    "#     max_date = group['Date'].max()\n",
    "#     group['Time_Diff_Days'] = (max_date - group['Date']).dt.days\n",
    "\n",
    "#     min_weight = 0.1\n",
    "#     max_weight = 0.9\n",
    "#     decay_factor = 0.5\n",
    "\n",
    "#     group['Weight'] = max_weight * np.exp(-decay_factor * group['Time_Diff_Days'])\n",
    "#     group['Weight'] = group['Weight'].clip(lower=min_weight, upper=max_weight)\n",
    "#     group['Weight'] = group['Weight'] / group['Weight'].sum()\n",
    "\n",
    "#     return group\n",
    "\n",
    "# def process_files(input_folder, output_excel):\n",
    "#     data = []\n",
    "\n",
    "#     for filename in os.listdir(input_folder):\n",
    "#         if filename.endswith(\".txt\"):\n",
    "#             with open(os.path.join(input_folder, filename), 'r', encoding='utf-8') as file:\n",
    "#                 lines = file.readlines()\n",
    "\n",
    "#                 stock = date = title = url = None\n",
    "#                 full_article_lines = []\n",
    "#                 collecting_article = False\n",
    "\n",
    "#                 for line in lines:\n",
    "#                     if line.startswith(\"Stock:\"):\n",
    "#                         stock = line.split(\":\", 1)[1].strip()\n",
    "#                     elif line.startswith(\"Date:\"):\n",
    "#                         date = line.split(\":\", 1)[1].strip()\n",
    "#                     elif line.startswith(\"Title:\"):\n",
    "#                         title = line.split(\":\", 1)[1].strip()\n",
    "#                     elif line.startswith(\"URL:\"):\n",
    "#                         url = line.split(\":\", 1)[1].strip()\n",
    "#                     elif line.startswith(\"Full Article:\"):\n",
    "#                         collecting_article = True\n",
    "#                     elif collecting_article:\n",
    "#                         full_article_lines.append(line.strip())\n",
    "\n",
    "#                 full_article = \"\\n\".join(full_article_lines).strip()\n",
    "\n",
    "#                 if not full_article or len(full_article.split()) < 5:\n",
    "#                     print(f\"Skipping file due to empty or short article: {filename}\")\n",
    "#                     continue\n",
    "\n",
    "#                 sentiment_score = perform_sentiment_analysis(full_article)\n",
    "\n",
    "#                 date = parse_date(date)\n",
    "#                 if not date:\n",
    "#                     print(f\"Skipping file due to invalid date: {filename}\")\n",
    "#                     continue\n",
    "\n",
    "#                 if stock and date and title and url:\n",
    "#                     data.append({\n",
    "#                         'Date': date,\n",
    "#                         'Stock': stock,\n",
    "#                         'Title': title,\n",
    "#                         'URL': url,\n",
    "#                         'Sentiment_score': sentiment_score\n",
    "#                     })\n",
    "#                 else:\n",
    "#                     print(f\"Skipping file due to missing fields: {filename}\")\n",
    "\n",
    "#     if data:\n",
    "#         df = pd.DataFrame(data)\n",
    "#         df.sort_values(by=[\"Stock\", \"Date\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "#         final_results = []\n",
    "\n",
    "#         for stock, group in df.groupby('Stock'):\n",
    "#             group = group.head(5).copy()\n",
    "\n",
    "#             group = assign_dynamic_weights(group)\n",
    "#             weighted_avg_score = (group['Sentiment_score'] * group['Weight']).sum()\n",
    "\n",
    "#             sentiment_std = group['Sentiment_score'].std() or 0\n",
    "\n",
    "#             if weighted_avg_score >= sentiment_std:\n",
    "#                 final_label_std = 'Positive'\n",
    "#             elif weighted_avg_score <= -sentiment_std:\n",
    "#                 final_label_std = 'Negative'\n",
    "#             else:\n",
    "#                 final_label_std = 'Neutral'\n",
    "\n",
    "#             if weighted_avg_score > 0.4:\n",
    "#                 final_label_threshold = 'Positive'\n",
    "#             elif weighted_avg_score < -0.4:\n",
    "#                 final_label_threshold = 'Negative'\n",
    "#             else:\n",
    "#                 final_label_threshold = 'Neutral'\n",
    "\n",
    "#             for _, row in group.iterrows():\n",
    "#                 final_results.append({\n",
    "#                     'Date': row['Date'],\n",
    "#                     'Stock': stock,\n",
    "#                     'Title': row['Title'],\n",
    "#                     'URL': row['URL'],\n",
    "#                     'Positive_score': max(row['Sentiment_score'], 0),\n",
    "#                     'Negative_score': abs(min(row['Sentiment_score'], 0)),\n",
    "#                     'Sentiment_score': row['Sentiment_score'],\n",
    "#                     'Weightage': row['Weight'],\n",
    "#                     'Weighted_average_score': weighted_avg_score,\n",
    "#                     'Final_sentiment_label_on_std': final_label_std,\n",
    "#                     'Final_sentiment_label_on_threshold_value': final_label_threshold\n",
    "#                 })\n",
    "\n",
    "#         final_df = pd.DataFrame(final_results)\n",
    "#         final_df['Date'] = final_df['Date'].dt.strftime('%d %b %Y %H:%M')\n",
    "#         final_df.to_excel(output_excel, index=False)\n",
    "\n",
    "#         print(f\"Output saved to {output_excel}\")\n",
    "\n",
    "#     else:\n",
    "#         print(\"No valid data to process.\")\n",
    "\n",
    "# input_folder = 'news_articles'\n",
    "# output_excel = 'filtered_output1.xlsx'\n",
    "\n",
    "# process_files(input_folder, output_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62ddbfc2-e3ca-4cbb-8f54-cc4e41e9a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import random\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urlencode, urlparse, parse_qs\n",
    "# from datetime import datetime, timedelta\n",
    "# from newspaper import Article\n",
    "# import pandas as pd\n",
    "# import yfinance as yf\n",
    "\n",
    "# # Stocks to scrape news for\n",
    "# stocks = [\"XOM\", \"CVX\", \"COP\", \"SLB\", \"EOG\", \"OXY\", \"VLO\", \"MPC\", \"PSX\",\n",
    "#           \"DOW\", \"DD\", \"LIN\", \"APD\", \"SHW\", \"NEM\", \"FCX\", \"ALB\", \"IP\", \"ECL\",\n",
    "#           \"GE\", \"HON\", \"UNP\", \"BA\", \"MMM\", \"CAT\", \"LMT\", \"RTX\", \"NOC\", \"DE\",\n",
    "#           \"AMZN\", \"TSLA\", \"HD\", \"MCD\", \"NKE\", \"SBUX\", \"BKNG\", \"LOW\", \"TGT\", \"GM\",\n",
    "#           \"PG\", \"KO\", \"PEP\", \"WMT\", \"COST\", \"PM\", \"MO\", \"MDLZ\", \"CL\", \"KMB\",\n",
    "#           \"JNJ\", \"PFE\", \"MRK\", \"ABBV\", \"UNH\", \"AMGN\", \"BMY\", \"GILD\", \"LLY\", \"ANTM\",\n",
    "#           \"JPM\", \"BAC\", \"WFC\", \"C\", \"GS\", \"MS\", \"AXP\", \"USB\", \"PNC\", \"SCHW\",\n",
    "#           \"AAPL\", \"MSFT\", \"GOOGL\", \"NVDA\", \"META\", \"INTC\", \"CSCO\", \"ORCL\", \"ADBE\", \"CRM\",\n",
    "#           \"GOOGL\", \"META\", \"VZ\", \"T\", \"CMCSA\", \"DIS\", \"NFLX\", \"CHTR\", \"TMUS\", \"ATVI\",\n",
    "#           \"NEE\", \"DUK\", \"SO\", \"D\", \"AEP\", \"EXC\", \"SRE\", \"PEG\", \"ED\", \"XEL\",\n",
    "#           \"AMT\", \"PLD\", \"CCI\", \"EQIX\", \"PSA\", \"SPG\", \"WELL\", \"DLR\", \"O\", \"AVB\"]\n",
    "\n",
    "# GOOGLE_NEWS_URL = \"https://www.google.com/search\"\n",
    "# HEADERS = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "# }\n",
    "# OUTPUT_DIR = \"news_articles\"\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# FETCH_LIMIT = 15\n",
    "# MAX_VALID_ARTICLES = 5\n",
    "\n",
    "\n",
    "# def fetch_full_article(url):\n",
    "#     try:\n",
    "#         article = Article(url)\n",
    "#         article.download()\n",
    "#         article.parse()\n",
    "#         return article.text\n",
    "#     except Exception as e:\n",
    "#         return \"\"\n",
    "\n",
    "\n",
    "# def clean_google_url(google_url):\n",
    "#     parsed_url = urlparse(google_url)\n",
    "#     query_params = parse_qs(parsed_url.query)\n",
    "#     return query_params.get(\"q\", [google_url])[0]\n",
    "\n",
    "\n",
    "# def convert_relative_date(relative_date):\n",
    "#     now = datetime.today()\n",
    "#     if \"hour\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(hours=num)\n",
    "#     elif \"day\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(days=num)\n",
    "#     elif \"week\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(weeks=num)\n",
    "#     elif \"minute\" in relative_date:\n",
    "#         article_date = now\n",
    "#     else:\n",
    "#         article_date = now\n",
    "#     return article_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# def scrape_news(stock):\n",
    "#     print(f\"Fetching news for: {stock}...\")\n",
    "#     valid_articles = []\n",
    "#     params = {\"q\": f\"{stock} stock news\", \"tbm\": \"nws\", \"hl\": \"en\", \"gl\": \"us\"}\n",
    "#     url = f\"{GOOGLE_NEWS_URL}?{urlencode(params)}\"\n",
    "#     response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to fetch news for {stock}\")\n",
    "#         return []\n",
    "\n",
    "#     soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#     articles_fetched = 0\n",
    "\n",
    "#     for article in soup.select(\".SoaBEf\"):\n",
    "#         if articles_fetched >= FETCH_LIMIT:\n",
    "#             break\n",
    "\n",
    "#         title_element = article.select_one(\".nDgy9d\")\n",
    "#         url_element = article.select_one(\"a\")\n",
    "#         date_element = article.select_one(\".OSrXXb\")\n",
    "\n",
    "#         title = title_element.text.strip() if title_element else \"No Title\"\n",
    "#         url = url_element[\"href\"] if url_element else \"\"\n",
    "#         raw_date = date_element.text.strip() if date_element else \"Unknown Date\"\n",
    "\n",
    "#         if url.startswith(\"/url?\"):\n",
    "#             url = clean_google_url(url)\n",
    "\n",
    "#         full_article = fetch_full_article(url)\n",
    "#         if not full_article.strip():\n",
    "#             continue\n",
    "\n",
    "#         valid_articles.append({\n",
    "#             \"title\": title,\n",
    "#             \"url\": url,\n",
    "#             \"date\": convert_relative_date(raw_date),\n",
    "#             \"full_article\": full_article,\n",
    "#         })\n",
    "\n",
    "#         articles_fetched += 1\n",
    "\n",
    "#         if len(valid_articles) >= MAX_VALID_ARTICLES:\n",
    "#             break\n",
    "\n",
    "#     return valid_articles\n",
    "\n",
    "\n",
    "# def save_articles(stock, articles):\n",
    "#     saved_dates = []\n",
    "#     date_counter = {}\n",
    "\n",
    "#     for i, article in enumerate(articles):\n",
    "#         base_filename = f\"{article['date']}_{stock}\"\n",
    "\n",
    "#         if article['date'] not in date_counter:\n",
    "#             date_counter[article['date']] = 1\n",
    "#         else:\n",
    "#             date_counter[article['date']] += 1\n",
    "\n",
    "#         if date_counter[article['date']] > 1:\n",
    "#             filename = f\"{base_filename}_{date_counter[article['date']]}.txt\"\n",
    "#         else:\n",
    "#             filename = f\"{base_filename}.txt\"\n",
    "\n",
    "#         filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "#         with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(f\"Stock: {stock}\\n\")\n",
    "#             f.write(f\"Date: {article['date']}\\n\")\n",
    "#             f.write(f\"Title: {article['title']}\\n\")\n",
    "#             f.write(f\"URL: {article['url']}\\n\")\n",
    "#             f.write(f\"Full Article:\\n{article['full_article']}\\n\")\n",
    "\n",
    "#         saved_dates.append(article['date'])\n",
    "#         print(f\"Saved: {filepath}\")\n",
    "\n",
    "#     return saved_dates\n",
    "\n",
    "\n",
    "# def fetch_ohlc_for_latest_date(stock, latest_date):\n",
    "#     stock_data = []\n",
    "\n",
    "#     try:\n",
    "#         start_date = datetime.strptime(latest_date, \"%Y-%m-%d\")\n",
    "#         end_date = start_date + timedelta(days=1)\n",
    "\n",
    "#         ticker = yf.Ticker(stock)\n",
    "#         df = ticker.history(start=start_date, end=end_date)\n",
    "\n",
    "#         if not df.empty:\n",
    "#             stock_data.append({\n",
    "#                 \"Date\": latest_date,\n",
    "#                 \"Stock\": stock,\n",
    "#                 \"Open_Price\": df.iloc[0][\"Open\"],\n",
    "#                 \"Close_Price\": df.iloc[0][\"Close\"],\n",
    "#                 \"High_Price\": df.iloc[0][\"High\"],\n",
    "#                 \"Low_Price\": df.iloc[0][\"Low\"],\n",
    "#                 \"Volume\": df.iloc[0][\"Volume\"],\n",
    "#             })\n",
    "#             print(f\"Fetched OHLC data for {stock} on {latest_date}\")\n",
    "#         else:\n",
    "#             print(f\"No OHLC data for {stock} on {latest_date}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to fetch OHLC data for {stock} on {latest_date}: {e}\")\n",
    "\n",
    "#     return stock_data\n",
    "\n",
    "\n",
    "# all_ohlc_data = []\n",
    "\n",
    "# for stock in stocks:\n",
    "#     articles = scrape_news(stock)\n",
    "#     if articles:\n",
    "#         saved_dates = save_articles(stock, articles[:MAX_VALID_ARTICLES])\n",
    "\n",
    "#         # Fetch OHLC for the most recent date only\n",
    "#         most_recent_date = max(saved_dates) if saved_dates else None\n",
    "\n",
    "#         if most_recent_date:\n",
    "#             ohlc_data = fetch_ohlc_for_latest_date(stock, most_recent_date)\n",
    "#             all_ohlc_data.extend(ohlc_data)\n",
    "\n",
    "#     else:\n",
    "#         print(f\"No valid articles found for {stock}.\")\n",
    "\n",
    "#     time.sleep(random.uniform(5, 10))\n",
    "\n",
    "# if all_ohlc_data:\n",
    "#     ohlc_df = pd.DataFrame(all_ohlc_data)\n",
    "#     ohlc_df.sort_values(by=[\"Date\", \"Stock\"], inplace=True)\n",
    "#     ohlc_df.to_excel(\"stock_ohlc_data.xlsx\", index=False)\n",
    "#     print(\"OHLC data saved to 'stock_ohlc_data.xlsx'\")\n",
    "# else:\n",
    "#     print(\"No OHLC data to save.\")\n",
    "\n",
    "# print(\"News scraping and OHLC data fetching completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ca43c4b-3892-422f-ba26-e4cebfc4e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# from datetime import datetime\n",
    "\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# def perform_sentiment_analysis(article_text):\n",
    "#     vader_scores = analyzer.polarity_scores(article_text)\n",
    "#     return vader_scores['compound']\n",
    "\n",
    "# def parse_date(date_string):\n",
    "#     formats = ['%Y-%m-%d', '%d-%m-%Y', '%d/%m/%Y']\n",
    "#     for fmt in formats:\n",
    "#         try:\n",
    "#             return datetime.strptime(date_string, fmt)\n",
    "#         except ValueError:\n",
    "#             continue\n",
    "#     print(f\"Invalid date format: {date_string}\")\n",
    "#     return None\n",
    "\n",
    "# def assign_dynamic_weights(group):\n",
    "#     max_date = group['Date'].max()\n",
    "#     group['Time_Diff_Days'] = (max_date - group['Date']).dt.days\n",
    "\n",
    "#     min_weight = 0.1\n",
    "#     max_weight = 0.9\n",
    "#     decay_factor = 0.5\n",
    "\n",
    "#     group['Weight'] = max_weight * np.exp(-decay_factor * group['Time_Diff_Days'])\n",
    "#     group['Weight'] = group['Weight'].clip(lower=min_weight, upper=max_weight)\n",
    "#     group['Weight'] = group['Weight'] / group['Weight'].sum()\n",
    "\n",
    "#     return group\n",
    "\n",
    "# def process_files(input_folder, output_excel, ohlc_file, final_output_with_ohlc):\n",
    "#     data = []\n",
    "\n",
    "#     for filename in os.listdir(input_folder):\n",
    "#         if filename.endswith(\".txt\"):\n",
    "#             with open(os.path.join(input_folder, filename), 'r', encoding='utf-8') as file:\n",
    "#                 lines = file.readlines()\n",
    "\n",
    "#                 stock = date = title = url = None\n",
    "#                 full_article_lines = []\n",
    "#                 collecting_article = False\n",
    "\n",
    "#                 for line in lines:\n",
    "#                     if line.startswith(\"Stock:\"):\n",
    "#                         stock = line.split(\":\", 1)[1].strip()\n",
    "#                     elif line.startswith(\"Date:\"):\n",
    "#                         date = line.split(\":\", 1)[1].strip()\n",
    "#                     elif line.startswith(\"Title:\"):\n",
    "#                         title = line.split(\":\", 1)[1].strip()\n",
    "#                     elif line.startswith(\"URL:\"):\n",
    "#                         url = line.split(\":\", 1)[1].strip()\n",
    "#                     elif line.startswith(\"Full Article:\"):\n",
    "#                         collecting_article = True\n",
    "#                     elif collecting_article:\n",
    "#                         full_article_lines.append(line.strip())\n",
    "\n",
    "#                 full_article = \"\\n\".join(full_article_lines).strip()\n",
    "\n",
    "#                 if not full_article or len(full_article.split()) < 5:\n",
    "#                     print(f\"Skipping file due to empty or short article: {filename}\")\n",
    "#                     continue\n",
    "\n",
    "#                 sentiment_score = perform_sentiment_analysis(full_article)\n",
    "\n",
    "#                 date = parse_date(date)\n",
    "#                 if not date:\n",
    "#                     print(f\"Skipping file due to invalid date: {filename}\")\n",
    "#                     continue\n",
    "\n",
    "#                 if stock and date and title and url:\n",
    "#                     data.append({\n",
    "#                         'Date': date,\n",
    "#                         'Stock': stock,\n",
    "#                         'Title': title,\n",
    "#                         'URL': url,\n",
    "#                         'Sentiment_score': sentiment_score\n",
    "#                     })\n",
    "#                 else:\n",
    "#                     print(f\"Skipping file due to missing fields: {filename}\")\n",
    "\n",
    "#     if data:\n",
    "#         df = pd.DataFrame(data)\n",
    "#         df.sort_values(by=[\"Stock\", \"Date\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "#         final_results = []\n",
    "\n",
    "#         for stock, group in df.groupby('Stock'):\n",
    "#             group = group.head(5).copy()\n",
    "#             group = assign_dynamic_weights(group)\n",
    "\n",
    "#             weighted_avg_score = (group['Sentiment_score'] * group['Weight']).sum()\n",
    "#             sentiment_std = group['Sentiment_score'].std() or 0\n",
    "\n",
    "#             if weighted_avg_score >= sentiment_std:\n",
    "#                 final_label_std = 'Positive'\n",
    "#             elif weighted_avg_score <= -sentiment_std:\n",
    "#                 final_label_std = 'Negative'\n",
    "#             else:\n",
    "#                 final_label_std = 'Neutral'\n",
    "\n",
    "#             if weighted_avg_score > 0.4:\n",
    "#                 final_label_threshold = 'Positive'\n",
    "#             elif weighted_avg_score < -0.4:\n",
    "#                 final_label_threshold = 'Negative'\n",
    "#             else:\n",
    "#                 final_label_threshold = 'Neutral'\n",
    "\n",
    "#             for _, row in group.iterrows():\n",
    "#                 final_results.append({\n",
    "#                     'Date': row['Date'],\n",
    "#                     'Stock': stock,\n",
    "#                     'Title': row['Title'],\n",
    "#                     'URL': row['URL'],\n",
    "#                     'Positive_score': max(row['Sentiment_score'], 0),\n",
    "#                     'Negative_score': abs(min(row['Sentiment_score'], 0)),\n",
    "#                     'Sentiment_score': row['Sentiment_score'],\n",
    "#                     'Weightage': row['Weight'],\n",
    "#                     'Weighted_average_score': weighted_avg_score,\n",
    "#                     'Final_sentiment_label_on_std': final_label_std,\n",
    "#                     'Final_sentiment_label_on_threshold_value': final_label_threshold\n",
    "#                 })\n",
    "\n",
    "#         final_df = pd.DataFrame(final_results)\n",
    "#         final_df['Date'] = final_df['Date'].dt.strftime('%d %b %Y %H:%M')\n",
    "#         final_df.to_excel(output_excel, index=False)\n",
    "\n",
    "#         print(f\"Output saved to {output_excel}\")\n",
    "\n",
    "#         # Load OHLC Data\n",
    "#         ohlc_df = pd.read_excel(ohlc_file)\n",
    "#         ohlc_df['Date'] = pd.to_datetime(ohlc_df['Date']).dt.date\n",
    "\n",
    "#         # Merge with Analysis Results\n",
    "#         final_df['Date'] = pd.to_datetime(final_df['Date'], format='%d %b %Y %H:%M').dt.date\n",
    "#         merged_df = pd.merge(final_df, ohlc_df, on=['Date', 'Stock'], how='left')\n",
    "\n",
    "#         # Fill missing values for each stock\n",
    "#         merged_df.sort_values(by=['Stock', 'Date'], inplace=True)\n",
    "#         merged_df[['Open_Price', 'Close_Price', 'High_Price', 'Low_Price', 'Volume']] = merged_df.groupby('Stock')[['Open_Price', 'Close_Price', 'High_Price', 'Low_Price', 'Volume']].ffill().bfill()\n",
    "\n",
    "#         merged_df.to_excel(final_output_with_ohlc, index=False)\n",
    "#         print(f\"Final output with OHLC data saved to '{final_output_with_ohlc}'\")\n",
    "\n",
    "#     else:\n",
    "#         print(\"No valid data to process.\")\n",
    "\n",
    "# input_folder = 'news_articles'\n",
    "# output_excel = 'filtered_output1.xlsx'\n",
    "# ohlc_file = 'stock_ohlc_data.xlsx'\n",
    "# final_output_with_ohlc = 'final_output_with_ohlc.xlsx'\n",
    "\n",
    "# process_files(input_folder, output_excel, ohlc_file, final_output_with_ohlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70aa9a2d-ccae-4f15-8f72-e1d8ba5e5087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import random\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urlencode, urlparse, parse_qs\n",
    "# from datetime import datetime, timedelta\n",
    "# from newspaper import Article\n",
    "# import pandas as pd\n",
    "# import yfinance as yf\n",
    "\n",
    "# # User-Agent rotation\n",
    "# USER_AGENTS = [\n",
    "#     \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "#     \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "#     \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "#     \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:96.0) Gecko/20100101 Firefox/96.0\",\n",
    "#     \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.192 Safari/537.36\",\n",
    "# ]\n",
    "\n",
    "# # Stocks to scrape news for\n",
    "# stocks = [\"GOOGL\", \"META\", \"VZ\", \"T\", \"CMCSA\", \"DIS\", \"NFLX\", \"CHTR\", \"TMUS\", \"ATVI\",\n",
    "#           \"NEE\", \"DUK\", \"SO\", \"D\", \"AEP\", \"EXC\", \"SRE\", \"PEG\", \"ED\", \"XEL\",\n",
    "#           \"AMT\", \"PLD\", \"CCI\", \"EQIX\", \"PSA\", \"SPG\", \"WELL\", \"DLR\", \"O\", \"AVB\"]\n",
    "\n",
    "# GOOGLE_NEWS_URL = \"https://www.google.com/search\"\n",
    "# OUTPUT_DIR = \"news_articles\"\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# FETCH_LIMIT = 15\n",
    "# MAX_VALID_ARTICLES = 5\n",
    "\n",
    "# def get_random_headers():\n",
    "#     return {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "\n",
    "# def fetch_full_article(url):\n",
    "#     try:\n",
    "#         article = Article(url)\n",
    "#         article.download()\n",
    "#         article.parse()\n",
    "#         return article.text\n",
    "#     except Exception as e:\n",
    "#         return \"\"\n",
    "\n",
    "# def clean_google_url(google_url):\n",
    "#     parsed_url = urlparse(google_url)\n",
    "#     query_params = parse_qs(parsed_url.query)\n",
    "#     return query_params.get(\"q\", [google_url])[0]\n",
    "\n",
    "# def convert_relative_date(relative_date):\n",
    "#     now = datetime.today()\n",
    "#     if \"hour\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(hours=num)\n",
    "#     elif \"day\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(days=num)\n",
    "#     elif \"week\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(weeks=num)\n",
    "#     elif \"minute\" in relative_date:\n",
    "#         article_date = now\n",
    "#     else:\n",
    "#         article_date = now\n",
    "#     return article_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# def scrape_news(stock):\n",
    "#     print(f\"\\nFetching news for: {stock}...\")\n",
    "#     valid_articles = []\n",
    "#     params = {\"q\": f\"{stock} stock news\", \"tbm\": \"nws\", \"hl\": \"en\", \"gl\": \"us\"}\n",
    "#     url = f\"{GOOGLE_NEWS_URL}?{urlencode(params)}\"\n",
    "#     headers = get_random_headers()\n",
    "#     response = requests.get(url, headers=headers)\n",
    "\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to fetch news for {stock}\")\n",
    "#         return []\n",
    "\n",
    "#     soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#     articles_fetched = 0\n",
    "\n",
    "#     for article in soup.select(\".SoaBEf\"):\n",
    "#         if articles_fetched >= FETCH_LIMIT:\n",
    "#             break\n",
    "\n",
    "#         title_element = article.select_one(\".nDgy9d\")\n",
    "#         url_element = article.select_one(\"a\")\n",
    "#         date_element = article.select_one(\".OSrXXb\")\n",
    "\n",
    "#         title = title_element.text.strip() if title_element else \"No Title\"\n",
    "#         url = url_element[\"href\"] if url_element else \"\"\n",
    "#         raw_date = date_element.text.strip() if date_element else \"Unknown Date\"\n",
    "\n",
    "#         if url.startswith(\"/url?\"):\n",
    "#             url = clean_google_url(url)\n",
    "\n",
    "#         full_article = fetch_full_article(url)\n",
    "#         if not full_article.strip():\n",
    "#             continue\n",
    "\n",
    "#         valid_articles.append({\n",
    "#             \"title\": title,\n",
    "#             \"url\": url,\n",
    "#             \"date\": convert_relative_date(raw_date),\n",
    "#             \"full_article\": full_article,\n",
    "#         })\n",
    "\n",
    "#         articles_fetched += 1\n",
    "\n",
    "#         if len(valid_articles) >= MAX_VALID_ARTICLES:\n",
    "#             break\n",
    "\n",
    "#     return valid_articles\n",
    "\n",
    "# def save_articles(stock, articles):\n",
    "#     saved_files = []\n",
    "\n",
    "#     for i, article in enumerate(articles):\n",
    "#         filename = f\"{article['date']}_{stock}.txt\"\n",
    "#         filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "#         counter = 1\n",
    "#         while os.path.exists(filepath):\n",
    "#             filepath = os.path.join(OUTPUT_DIR, f\"{article['date']}_{stock}_{counter}.txt\")\n",
    "#             counter += 1\n",
    "\n",
    "#         with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(f\"Stock: {stock}\\n\")\n",
    "#             f.write(f\"Date: {article['date']}\\n\")\n",
    "#             f.write(f\"Title: {article['title']}\\n\")\n",
    "#             f.write(f\"URL: {article['url']}\\n\")\n",
    "#             f.write(f\"Full Article:\\n{article['full_article']}\\n\")\n",
    "\n",
    "#         saved_files.append(filepath)\n",
    "#         print(f\"Saved: {filepath}\")\n",
    "\n",
    "#     return len(saved_files) > 0\n",
    "\n",
    "# def fetch_recent_ohlc(stock):\n",
    "#     try:\n",
    "#         ticker = yf.Ticker(stock)\n",
    "#         df = ticker.history(period=\"1d\")\n",
    "\n",
    "#         if not df.empty:\n",
    "#             data = {\n",
    "#                 \"Stock\": stock,\n",
    "#                 \"Open_Price\": df.iloc[-1][\"Open\"],\n",
    "#                 \"Close_Price\": df.iloc[-1][\"Close\"],\n",
    "#                 \"High_Price\": df.iloc[-1][\"High\"],\n",
    "#                 \"Low_Price\": df.iloc[-1][\"Low\"],\n",
    "#                 \"Volume\": df.iloc[-1][\"Volume\"],\n",
    "#             }\n",
    "#             print(f\"OHLC Data Fetched: {stock} - Close: {data['Close_Price']}\")\n",
    "#             return data\n",
    "#         else:\n",
    "#             print(f\"No OHLC data found for {stock}\")\n",
    "#             return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to fetch OHLC data for {stock}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# all_ohlc_data = []\n",
    "\n",
    "# for stock in stocks:\n",
    "#     articles = scrape_news(stock)\n",
    "#     saved_articles = save_articles(stock, articles) if articles else False\n",
    "\n",
    "#     if saved_articles:\n",
    "#         ohlc_data = fetch_recent_ohlc(stock)\n",
    "#         if ohlc_data:\n",
    "#             all_ohlc_data.append(ohlc_data)\n",
    "\n",
    "#     time.sleep(random.uniform(5, 10))\n",
    "\n",
    "# if all_ohlc_data:\n",
    "#     ohlc_df = pd.DataFrame(all_ohlc_data)\n",
    "#     ohlc_df.to_excel(\"stock_ohlc_data.xlsx\", index=False)\n",
    "#     print(\"\\nOHLC data saved to 'stock_ohlc_data.xlsx'\")\n",
    "# else:\n",
    "#     print(\"\\nNo OHLC data to save.\")\n",
    "\n",
    "# print(\"\\nNews scraping and OHLC data fetching completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9adf9c79-cdc3-40c9-8146-669775f7a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import random\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urlencode, urlparse, parse_qs\n",
    "# from datetime import datetime, timedelta\n",
    "# from newspaper import Article\n",
    "# import pandas as pd\n",
    "# import yfinance as yf\n",
    "\n",
    "# # Stocks to scrape news for\n",
    "# stocks = [\"ANTM\"\n",
    "#           # \"XOM\", \"CVX\", \"COP\", \"SLB\", \"EOG\", \"OXY\", \"VLO\", \"MPC\", \"PSX\",\n",
    "#           # \"DOW\", \"DD\", \"LIN\", \"APD\", \"SHW\", \"NEM\", \"FCX\", \"ALB\", \"IP\", \"ECL\",\n",
    "#           # \"GE\", \"HON\", \"UNP\", \"BA\", \"MMM\", \"CAT\", \"LMT\", \"RTX\", \"NOC\", \"DE\",\n",
    "#           # \"AMZN\", \"TSLA\", \"HD\", \"MCD\", \"NKE\", \"SBUX\", \"BKNG\", \"LOW\", \"TGT\", \"GM\",\n",
    "#           # \"PG\", \"KO\", \"PEP\", \"WMT\", \"COST\", \"PM\", \"MO\", \"MDLZ\", \"CL\", \"KMB\",\n",
    "#           # \"JNJ\", \"PFE\", \"MRK\", \"ABBV\", \"UNH\", \"AMGN\", \"BMY\", \"GILD\", \"LLY\", \"ANTM\",\n",
    "#           # \"JPM\", \"BAC\", \"WFC\", \"C\", \"GS\", \"MS\", \"AXP\", \"USB\", \"PNC\", \"SCHW\",\n",
    "#           # \"AAPL\", \"MSFT\", \"GOOGL\", \"NVDA\", \"META\", \"INTC\", \"CSCO\", \"ORCL\", \"ADBE\", \"CRM\",\n",
    "#           # \"GOOGL\", \"META\", \"VZ\", \"T\", \"CMCSA\", \"DIS\", \"NFLX\", \"CHTR\", \"TMUS\", \"ATVI\",\n",
    "#           # \"NEE\", \"DUK\", \"SO\", \"D\", \"AEP\", \"EXC\", \"SRE\", \"PEG\", \"ED\", \"XEL\",\n",
    "#           # \"AMT\", \"PLD\", \"CCI\", \"EQIX\", \"PSA\", \"SPG\", \"WELL\", \"DLR\", \"O\", \"AVB\"\n",
    "#          ]\n",
    "\n",
    "# GOOGLE_NEWS_URL = \"https://www.google.com/search\"\n",
    "# HEADERS = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "# }\n",
    "# OUTPUT_DIR = \"News_articles\"\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# FETCH_LIMIT = 15\n",
    "# MAX_VALID_ARTICLES = 5\n",
    "\n",
    "\n",
    "# def fetch_full_article(url):\n",
    "#     try:\n",
    "#         article = Article(url)\n",
    "#         article.download()\n",
    "#         article.parse()\n",
    "#         return article.text\n",
    "#     except Exception as e:\n",
    "#         return \"\"\n",
    "\n",
    "\n",
    "# def clean_google_url(google_url):\n",
    "#     parsed_url = urlparse(google_url)\n",
    "#     query_params = parse_qs(parsed_url.query)\n",
    "#     return query_params.get(\"q\", [google_url])[0]\n",
    "\n",
    "\n",
    "# def convert_relative_date(relative_date):\n",
    "#     now = datetime.today()\n",
    "#     if \"hour\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(hours=num)\n",
    "#     elif \"day\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(days=num)\n",
    "#     elif \"week\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(weeks=num)\n",
    "#     elif \"minute\" in relative_date:\n",
    "#         article_date = now\n",
    "#     else:\n",
    "#         article_date = now\n",
    "#     return article_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# def scrape_news(stock):\n",
    "#     print(f\"\\nFetching news for: {stock}...\")\n",
    "#     valid_articles = []\n",
    "#     params = {\"q\": f\"{stock} stock news\", \"tbm\": \"nws\", \"hl\": \"en\", \"gl\": \"us\"}\n",
    "#     url = f\"{GOOGLE_NEWS_URL}?{urlencode(params)}\"\n",
    "#     response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to fetch news for {stock}\")\n",
    "#         return []\n",
    "\n",
    "#     soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#     articles_fetched = 0\n",
    "\n",
    "#     for article in soup.select(\".SoaBEf\"):\n",
    "#         if articles_fetched >= FETCH_LIMIT:\n",
    "#             break\n",
    "\n",
    "#         title_element = article.select_one(\".nDgy9d\")\n",
    "#         url_element = article.select_one(\"a\")\n",
    "#         date_element = article.select_one(\".OSrXXb\")\n",
    "\n",
    "#         title = title_element.text.strip() if title_element else \"No Title\"\n",
    "#         url = url_element[\"href\"] if url_element else \"\"\n",
    "#         raw_date = date_element.text.strip() if date_element else \"Unknown Date\"\n",
    "\n",
    "#         if url.startswith(\"/url?\"):\n",
    "#             url = clean_google_url(url)\n",
    "\n",
    "#         full_article = fetch_full_article(url)\n",
    "#         if not full_article.strip():\n",
    "#             continue\n",
    "\n",
    "#         valid_articles.append({\n",
    "#             \"title\": title,\n",
    "#             \"url\": url,\n",
    "#             \"date\": convert_relative_date(raw_date),\n",
    "#             \"full_article\": full_article,\n",
    "#         })\n",
    "\n",
    "#         articles_fetched += 1\n",
    "\n",
    "#         if len(valid_articles) >= MAX_VALID_ARTICLES:\n",
    "#             break\n",
    "\n",
    "#     return valid_articles\n",
    "\n",
    "\n",
    "# def save_articles(stock, articles):\n",
    "#     saved_files = []\n",
    "\n",
    "#     for i, article in enumerate(articles):\n",
    "#         filename = f\"{article['date']}_{stock}.txt\"\n",
    "#         filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "#         # Handle duplicate file names (appending _1, _2, etc.)\n",
    "#         counter = 1\n",
    "#         while os.path.exists(filepath):\n",
    "#             filepath = os.path.join(OUTPUT_DIR, f\"{article['date']}_{stock}_{counter}.txt\")\n",
    "#             counter += 1\n",
    "\n",
    "#         with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(f\"Stock: {stock}\\n\")\n",
    "#             f.write(f\"Date: {article['date']}\\n\")\n",
    "#             f.write(f\"Title: {article['title']}\\n\")\n",
    "#             f.write(f\"URL: {article['url']}\\n\")\n",
    "#             f.write(f\"Full Article:\\n{article['full_article']}\\n\")\n",
    "\n",
    "#         saved_files.append(filepath)\n",
    "#         print(f\"Saved: {filepath}\")\n",
    "\n",
    "#     return len(saved_files) > 0\n",
    "\n",
    "\n",
    "# def fetch_recent_ohlc(stock):\n",
    "#     try:\n",
    "#         ticker = yf.Ticker(stock)\n",
    "#         periods = [\"1d\", \"5d\", \"1mo\", \"3mo\"]  # Check different periods in order\n",
    "#         data = None\n",
    "\n",
    "#         for period in periods:\n",
    "#             df = ticker.history(period=period)\n",
    "#             if not df.empty:\n",
    "#                 latest_row = df.iloc[-1]  # Get the most recent row\n",
    "#                 data = {\n",
    "#                     \"Stock\": stock,\n",
    "#                     \"Open_Price\": latest_row[\"Open\"],\n",
    "#                     \"Close_Price\": latest_row[\"Close\"],\n",
    "#                     \"High_Price\": latest_row[\"High\"],\n",
    "#                     \"Low_Price\": latest_row[\"Low\"],\n",
    "#                     \"Volume\": latest_row[\"Volume\"],\n",
    "#                     \"Date\": latest_row.name.strftime(\"%Y-%m-%d\"),\n",
    "#                 }\n",
    "#                 print(f\"OHLC Data Found: {stock} - Period: {period} - Close: {data['Close_Price']}\")\n",
    "#                 break  # Stop after finding the first available period\n",
    "\n",
    "#         if not data:\n",
    "#             print(f\"No OHLC data found for {stock} in any checked periods.\")\n",
    "\n",
    "#         return data\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to fetch OHLC data for {stock}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# all_ohlc_data = []\n",
    "\n",
    "# for stock in stocks:\n",
    "#     # 1. Scrape and Save News Articles\n",
    "#     articles = scrape_news(stock)\n",
    "#     saved_articles = save_articles(stock, articles) if articles else False\n",
    "\n",
    "#     # 2. Fetch OHLC Data only if news was saved\n",
    "#     if saved_articles:\n",
    "#         ohlc_data = fetch_recent_ohlc(stock)\n",
    "#         if ohlc_data:\n",
    "#             all_ohlc_data.append(ohlc_data)\n",
    "\n",
    "#     # Pause between requests\n",
    "#     time.sleep(random.uniform(5, 10))\n",
    "\n",
    "# # Save OHLC Data to Excel\n",
    "# if all_ohlc_data:\n",
    "#     ohlc_df = pd.DataFrame(all_ohlc_data)\n",
    "#     ohlc_df.to_excel(\"Stock_ohlc_data_Ex.xlsx\", index=False)\n",
    "#     print(\"\\n OHLC data saved to 'stock_ohlc_data.xlsx'\")\n",
    "# else:\n",
    "#     print(\"\\n No OHLC data to save.\")\n",
    "\n",
    "# print(\"\\n News scraping and OHLC data fetching completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a71ac39-90a9-4a13-9650-6e6d4be89af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import random\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urlencode, urlparse, parse_qs\n",
    "# from datetime import datetime, timedelta\n",
    "# from newspaper import Article\n",
    "# import pandas as pd\n",
    "# import yfinance as yf\n",
    "\n",
    "# # Stocks to scrape news for\n",
    "# stocks = [# \"INFY\"\n",
    "#           \"XOM\", \"CVX\", \"COP\", \"SLB\", \"EOG\", \"OXY\", \"VLO\", \"MPC\", \"PSX\",\n",
    "#           \"DOW\", \"DD\", \"LIN\", \"APD\", \"SHW\", \"NEM\", \"FCX\", \"ALB\", \"IP\", \"ECL\",\n",
    "#           \"GE\", \"HON\", \"UNP\", \"BA\", \"MMM\", \"CAT\", \"LMT\", \"RTX\", \"NOC\", \"DE\",\n",
    "#           \"AMZN\", \"TSLA\", \"HD\", \"MCD\", \"NKE\", \"SBUX\", \"BKNG\", \"LOW\", \"TGT\", \"GM\",\n",
    "#           \"PG\", \"KO\", \"PEP\", \"WMT\", \"COST\", \"PM\", \"MO\", \"MDLZ\", \"CL\", \"KMB\",\n",
    "#           \"JNJ\", \"PFE\", \"MRK\", \"ABBV\", \"UNH\", \"AMGN\", \"BMY\", \"GILD\", \"LLY\", \"ANTM\",\n",
    "#           \"JPM\", \"BAC\", \"WFC\", \"C\", \"GS\", \"MS\", \"AXP\", \"USB\", \"PNC\", \"SCHW\",\n",
    "#           \"AAPL\", \"MSFT\", \"GOOGL\", \"NVDA\", \"META\", \"INTC\", \"CSCO\", \"ORCL\", \"ADBE\", \"CRM\",\n",
    "#           \"GOOGL\", \"META\", \"VZ\", \"T\", \"CMCSA\", \"DIS\", \"NFLX\", \"CHTR\", \"TMUS\", \"ATVI\",\n",
    "#           \"NEE\", \"DUK\", \"SO\", \"D\", \"AEP\", \"EXC\", \"SRE\", \"PEG\", \"ED\", \"XEL\",\n",
    "#           \"AMT\", \"PLD\", \"CCI\", \"EQIX\", \"PSA\", \"SPG\", \"WELL\", \"DLR\", \"O\", \"AVB\"\n",
    "#          ]\n",
    "\n",
    "# GOOGLE_NEWS_URL = \"https://www.google.com/search\"\n",
    "# HEADERS = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "# }\n",
    "# OUTPUT_DIR = \"news_articles\"\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# FETCH_LIMIT = 15\n",
    "# MAX_VALID_ARTICLES = 5\n",
    "\n",
    "\n",
    "# def fetch_full_article(url):\n",
    "#     try:\n",
    "#         article = Article(url)\n",
    "#         article.download()\n",
    "#         article.parse()\n",
    "#         return article.text\n",
    "#     except Exception as e:\n",
    "#         return \"\"\n",
    "\n",
    "\n",
    "# def clean_google_url(google_url):\n",
    "#     parsed_url = urlparse(google_url)\n",
    "#     query_params = parse_qs(parsed_url.query)\n",
    "#     return query_params.get(\"q\", [google_url])[0]\n",
    "\n",
    "\n",
    "# def convert_relative_date(relative_date):\n",
    "#     now = datetime.today()\n",
    "#     if \"hour\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(hours=num)\n",
    "#     elif \"day\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(days=num)\n",
    "#     elif \"week\" in relative_date:\n",
    "#         num = int(relative_date.split()[0])\n",
    "#         article_date = now - timedelta(weeks=num)\n",
    "#     elif \"minute\" in relative_date:\n",
    "#         article_date = now\n",
    "#     else:\n",
    "#         article_date = now\n",
    "#     return article_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# def scrape_news(stock):\n",
    "#     print(f\"\\nFetching news for: {stock}...\")\n",
    "#     valid_articles = []\n",
    "#     params = {\"q\": f\"{stock} stock news\", \"tbm\": \"nws\", \"hl\": \"en\", \"gl\": \"us\"}\n",
    "#     url = f\"{GOOGLE_NEWS_URL}?{urlencode(params)}\"\n",
    "#     response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to fetch news for {stock}\")\n",
    "#         return []\n",
    "\n",
    "#     soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#     articles_fetched = 0\n",
    "\n",
    "#     for article in soup.select(\".SoaBEf\"):\n",
    "#         if articles_fetched >= FETCH_LIMIT:\n",
    "#             break\n",
    "\n",
    "#         title_element = article.select_one(\".nDgy9d\")\n",
    "#         url_element = article.select_one(\"a\")\n",
    "#         date_element = article.select_one(\".OSrXXb\")\n",
    "\n",
    "#         title = title_element.text.strip() if title_element else \"No Title\"\n",
    "#         url = url_element[\"href\"] if url_element else \"\"\n",
    "#         raw_date = date_element.text.strip() if date_element else \"Unknown Date\"\n",
    "\n",
    "#         if url.startswith(\"/url?\"):\n",
    "#             url = clean_google_url(url)\n",
    "\n",
    "#         full_article = fetch_full_article(url)\n",
    "#         if not full_article.strip():\n",
    "#             continue\n",
    "\n",
    "#         valid_articles.append({\n",
    "#             \"title\": title,\n",
    "#             \"url\": url,\n",
    "#             \"date\": convert_relative_date(raw_date),\n",
    "#             \"full_article\": full_article,\n",
    "#         })\n",
    "\n",
    "#         articles_fetched += 1\n",
    "\n",
    "#         if len(valid_articles) >= MAX_VALID_ARTICLES:\n",
    "#             break\n",
    "\n",
    "#     return valid_articles\n",
    "\n",
    "\n",
    "# def save_articles(stock, articles):\n",
    "#     saved_files = []\n",
    "\n",
    "#     for i, article in enumerate(articles):\n",
    "#         filename = f\"{article['date']}_{stock}.txt\"\n",
    "#         filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "#         # Handle duplicate file names (appending _1, _2, etc.)\n",
    "#         counter = 1\n",
    "#         while os.path.exists(filepath):\n",
    "#             filepath = os.path.join(OUTPUT_DIR, f\"{article['date']}_{stock}_{counter}.txt\")\n",
    "#             counter += 1\n",
    "\n",
    "#         with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(f\"Stock: {stock}\\n\")\n",
    "#             f.write(f\"Date: {article['date']}\\n\")\n",
    "#             f.write(f\"Title: {article['title']}\\n\")\n",
    "#             f.write(f\"URL: {article['url']}\\n\")\n",
    "#             f.write(f\"Full Article:\\n{article['full_article']}\\n\")\n",
    "\n",
    "#         saved_files.append(filepath)\n",
    "#         print(f\"Saved: {filepath}\")\n",
    "\n",
    "#     return len(saved_files) > 0\n",
    "\n",
    "\n",
    "# def fetch_recent_ohlc(stock):\n",
    "#     try:\n",
    "#         ticker = yf.Ticker(stock)\n",
    "#         df = ticker.history(period=\"1d\")\n",
    "\n",
    "#         if not df.empty:\n",
    "#             data = {\n",
    "#                 \"Stock\": stock,\n",
    "#                 \"Open_Price\": df.iloc[-1][\"Open\"],\n",
    "#                 \"Close_Price\": df.iloc[-1][\"Close\"],\n",
    "#                 \"High_Price\": df.iloc[-1][\"High\"],\n",
    "#                 \"Low_Price\": df.iloc[-1][\"Low\"],\n",
    "#                 \"Volume\": df.iloc[-1][\"Volume\"],\n",
    "#             }\n",
    "#             print(f\"OHLC Data Fetched: {stock} - Close: {data['Close_Price']}\")\n",
    "#             return data\n",
    "#         else:\n",
    "#             print(f\" No OHLC data found for {stock}\")\n",
    "#             return None\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\" Failed to fetch OHLC data for {stock}: {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "# all_ohlc_data = []\n",
    "\n",
    "# for stock in stocks:\n",
    "#     # 1. Scrape and Save News Articles\n",
    "#     articles = scrape_news(stock)\n",
    "#     saved_articles = save_articles(stock, articles) if articles else False\n",
    "\n",
    "#     # 2. Fetch OHLC Data only if news was saved\n",
    "#     if saved_articles:\n",
    "#         ohlc_data = fetch_recent_ohlc(stock)\n",
    "#         if ohlc_data:\n",
    "#             all_ohlc_data.append(ohlc_data)\n",
    "\n",
    "#     # Pause between requests\n",
    "#     time.sleep(random.uniform(5, 10))\n",
    "\n",
    "# # Save OHLC Data to Excel\n",
    "# if all_ohlc_data:\n",
    "#     ohlc_df = pd.DataFrame(all_ohlc_data)\n",
    "#     ohlc_df.to_excel(\"stock_ohlc_data.xlsx\", index=False)\n",
    "#     print(\"\\n OHLC data saved to 'stock_ohlc_data.xlsx'\")\n",
    "# else:\n",
    "#     print(\"\\n No OHLC data to save.\")\n",
    "\n",
    "# print(\"\\n News scraping and OHLC data fetching completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf9510b-d357-4e11-bbe0-9716b713c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching news for: XOM...\n",
      "Saved: news_articles\\2025-02-16_XOM.txt\n",
      "Saved: news_articles\\2025-02-18_XOM.txt\n",
      "Saved: news_articles\\2025-02-17_XOM.txt\n",
      "Saved: news_articles\\2025-02-14_XOM.txt\n",
      "Saved: news_articles\\2025-02-18_XOM_1.txt\n",
      "OHLC Data Fetched: XOM - Close: 108.23999786376953\n",
      "\n",
      "Fetching news for: CVX...\n",
      "Saved: news_articles\\2025-02-15_CVX.txt\n",
      "Saved: news_articles\\2025-02-17_CVX.txt\n",
      "Saved: news_articles\\2025-02-15_CVX_1.txt\n",
      "Saved: news_articles\\2025-02-18_CVX.txt\n",
      "Saved: news_articles\\2025-02-18_CVX_1.txt\n",
      "OHLC Data Fetched: CVX - Close: 155.33999633789062\n",
      "\n",
      "Fetching news for: COP...\n",
      "Saved: news_articles\\2025-02-18_COP.txt\n",
      "Saved: news_articles\\2025-02-14_COP.txt\n",
      "Saved: news_articles\\2025-02-15_COP.txt\n",
      "Saved: news_articles\\2025-02-17_COP.txt\n",
      "Saved: news_articles\\2025-02-18_COP_1.txt\n",
      "OHLC Data Fetched: COP - Close: 96.26000213623047\n",
      "\n",
      "Fetching news for: SLB...\n",
      "Saved: news_articles\\2025-02-18_SLB.txt\n",
      "Saved: news_articles\\2025-02-15_SLB.txt\n",
      "Saved: news_articles\\2025-02-04_SLB.txt\n",
      "Saved: news_articles\\2025-02-18_SLB_1.txt\n",
      "Saved: news_articles\\2025-02-18_SLB_2.txt\n",
      "OHLC Data Fetched: SLB - Close: 41.75\n",
      "\n",
      "Fetching news for: EOG...\n",
      "Saved: news_articles\\2025-02-14_EOG.txt\n",
      "Saved: news_articles\\2025-02-17_EOG.txt\n",
      "Saved: news_articles\\2025-02-17_EOG_1.txt\n",
      "Saved: news_articles\\2025-02-15_EOG.txt\n",
      "Saved: news_articles\\2025-02-17_EOG_2.txt\n",
      "OHLC Data Fetched: EOG - Close: 129.30999755859375\n",
      "\n",
      "Fetching news for: OXY...\n",
      "Saved: news_articles\\2025-02-17_OXY.txt\n",
      "Saved: news_articles\\2025-02-17_OXY_1.txt\n",
      "Saved: news_articles\\2025-02-16_OXY.txt\n",
      "Saved: news_articles\\2025-02-15_OXY.txt\n",
      "Saved: news_articles\\2025-02-17_OXY_2.txt\n",
      "OHLC Data Fetched: OXY - Close: 48.060001373291016\n",
      "\n",
      "Fetching news for: VLO...\n",
      "Saved: news_articles\\2025-02-18_VLO.txt\n",
      "Saved: news_articles\\2025-01-28_VLO.txt\n",
      "Saved: news_articles\\2025-02-18_VLO_1.txt\n",
      "Saved: news_articles\\2025-02-16_VLO.txt\n",
      "Saved: news_articles\\2025-02-18_VLO_2.txt\n",
      "OHLC Data Fetched: VLO - Close: 135.30999755859375\n",
      "\n",
      "Fetching news for: MPC...\n",
      "Saved: news_articles\\2025-02-15_MPC.txt\n",
      "Saved: news_articles\\2025-02-17_MPC.txt\n",
      "Saved: news_articles\\2025-01-21_MPC.txt\n",
      "Saved: news_articles\\2025-02-04_MPC.txt\n",
      "Saved: news_articles\\2025-02-17_MPC_1.txt\n",
      "OHLC Data Fetched: MPC - Close: 156.05999755859375\n",
      "\n",
      "Fetching news for: PSX...\n",
      "Saved: news_articles\\2025-02-15_PSX.txt\n",
      "Saved: news_articles\\2025-02-13_PSX.txt\n",
      "Saved: news_articles\\2025-02-18_PSX.txt\n",
      "Saved: news_articles\\2025-02-14_PSX.txt\n",
      "Saved: news_articles\\2025-02-18_PSX_1.txt\n",
      "OHLC Data Fetched: PSX - Close: 128.0399932861328\n",
      "\n",
      "Fetching news for: DOW...\n",
      "Saved: news_articles\\2025-02-14_DOW.txt\n",
      "Saved: news_articles\\2025-02-14_DOW_1.txt\n",
      "Saved: news_articles\\2025-02-04_DOW.txt\n",
      "Saved: news_articles\\2025-02-04_DOW_1.txt\n",
      "Saved: news_articles\\2025-02-18_DOW.txt\n",
      "OHLC Data Fetched: DOW - Close: 38.66999816894531\n",
      "\n",
      "Fetching news for: DD...\n",
      "Saved: news_articles\\2025-02-18_DD.txt\n",
      "Saved: news_articles\\2025-02-16_DD.txt\n",
      "Saved: news_articles\\2025-02-18_DD_1.txt\n",
      "Saved: news_articles\\2025-02-18_DD_2.txt\n",
      "Saved: news_articles\\2025-02-18_DD_3.txt\n",
      "OHLC Data Fetched: DD - Close: 83.25\n",
      "\n",
      "Fetching news for: LIN...\n",
      "Saved: news_articles\\2025-02-18_LIN.txt\n",
      "Saved: news_articles\\2025-02-04_LIN.txt\n",
      "Saved: news_articles\\2025-02-18_LIN_1.txt\n",
      "Saved: news_articles\\2025-02-18_LIN_2.txt\n",
      "Saved: news_articles\\2025-02-18_LIN_3.txt\n",
      "OHLC Data Fetched: LIN - Close: 457.0\n",
      "\n",
      "Fetching news for: APD...\n",
      "Saved: news_articles\\2025-02-17_APD.txt\n",
      "Saved: news_articles\\2025-02-18_APD.txt\n",
      "Saved: news_articles\\2025-02-18_APD_1.txt\n",
      "Saved: news_articles\\2025-02-11_APD.txt\n",
      "Saved: news_articles\\2025-02-18_APD_2.txt\n",
      "OHLC Data Fetched: APD - Close: 316.1199951171875\n",
      "\n",
      "Fetching news for: SHW...\n",
      "Saved: news_articles\\2025-02-17_SHW.txt\n",
      "Saved: news_articles\\2025-02-17_SHW_1.txt\n",
      "Saved: news_articles\\2025-02-17_SHW_2.txt\n",
      "Saved: news_articles\\2025-01-28_SHW.txt\n",
      "Saved: news_articles\\2025-02-16_SHW.txt\n",
      "OHLC Data Fetched: SHW - Close: 356.8599853515625\n",
      "\n",
      "Fetching news for: NEM...\n",
      "Saved: news_articles\\2025-02-16_NEM.txt\n",
      "Saved: news_articles\\2025-02-18_NEM.txt\n",
      "Saved: news_articles\\2025-02-16_NEM_1.txt\n",
      "Saved: news_articles\\2025-02-18_NEM_1.txt\n",
      "Saved: news_articles\\2025-02-18_NEM_2.txt\n",
      "OHLC Data Fetched: NEM - Close: 46.540000915527344\n",
      "\n",
      "Fetching news for: FCX...\n",
      "Saved: news_articles\\2025-02-15_FCX.txt\n",
      "Saved: news_articles\\2025-02-16_FCX.txt\n",
      "Saved: news_articles\\2025-02-18_FCX.txt\n",
      "Saved: news_articles\\2025-02-16_FCX_1.txt\n",
      "Saved: news_articles\\2025-02-18_FCX_1.txt\n",
      "OHLC Data Fetched: FCX - Close: 39.470001220703125\n",
      "\n",
      "Fetching news for: ALB...\n",
      "Saved: news_articles\\2025-02-18_ALB.txt\n",
      "Saved: news_articles\\2025-02-13_ALB.txt\n",
      "Saved: news_articles\\2025-02-14_ALB.txt\n",
      "Saved: news_articles\\2025-02-04_ALB.txt\n",
      "Saved: news_articles\\2025-02-14_ALB_1.txt\n",
      "OHLC Data Fetched: ALB - Close: 81.20999908447266\n",
      "\n",
      "Fetching news for: IP...\n",
      "Saved: news_articles\\2025-01-28_IP.txt\n",
      "Saved: news_articles\\2025-02-18_IP.txt\n",
      "Saved: news_articles\\2025-02-04_IP.txt\n",
      "Saved: news_articles\\2025-02-18_IP_1.txt\n",
      "Saved: news_articles\\2025-01-28_IP_1.txt\n",
      "OHLC Data Fetched: IP - Close: 56.119998931884766\n",
      "\n",
      "Fetching news for: ECL...\n",
      "Saved: news_articles\\2025-02-18_ECL.txt\n",
      "Saved: news_articles\\2025-02-18_ECL_1.txt\n",
      "Saved: news_articles\\2025-02-13_ECL.txt\n",
      "Saved: news_articles\\2025-02-12_ECL.txt\n",
      "Saved: news_articles\\2025-02-18_ECL_2.txt\n",
      "OHLC Data Fetched: ECL - Close: 264.739990234375\n",
      "\n",
      "Fetching news for: GE...\n",
      "Saved: news_articles\\2025-02-18_GE.txt\n",
      "Saved: news_articles\\2025-01-21_GE.txt\n",
      "Saved: news_articles\\2025-02-04_GE.txt\n",
      "Saved: news_articles\\2025-02-18_GE_1.txt\n",
      "Saved: news_articles\\2025-02-17_GE.txt\n",
      "OHLC Data Fetched: GE - Close: 208.27000427246094\n",
      "\n",
      "Fetching news for: HON...\n",
      "Saved: news_articles\\2025-02-15_HON.txt\n",
      "Saved: news_articles\\2025-02-15_HON_1.txt\n",
      "Saved: news_articles\\2025-02-04_HON.txt\n",
      "Saved: news_articles\\2025-02-18_HON.txt\n",
      "Saved: news_articles\\2025-02-13_HON.txt\n",
      "OHLC Data Fetched: HON - Close: 202.75\n",
      "\n",
      "Fetching news for: UNP...\n",
      "Saved: news_articles\\2025-02-18_UNP.txt\n",
      "Saved: news_articles\\2025-02-17_UNP.txt\n",
      "Saved: news_articles\\2025-02-14_UNP.txt\n",
      "Saved: news_articles\\2025-01-28_UNP.txt\n",
      "Saved: news_articles\\2025-02-18_UNP_1.txt\n",
      "OHLC Data Fetched: UNP - Close: 249.22000122070312\n",
      "\n",
      "Fetching news for: BA...\n",
      "Saved: news_articles\\2025-02-16_BA.txt\n",
      "Saved: news_articles\\2025-02-18_BA.txt\n",
      "Saved: news_articles\\2025-02-18_BA_1.txt\n",
      "Saved: news_articles\\2025-02-13_BA.txt\n",
      "Saved: news_articles\\2025-02-18_BA_2.txt\n",
      "OHLC Data Fetched: BA - Close: 184.4199981689453\n",
      "\n",
      "Fetching news for: MMM...\n",
      "Saved: news_articles\\2025-02-12_MMM.txt\n",
      "Saved: news_articles\\2025-02-18_MMM.txt\n",
      "Saved: news_articles\\2025-02-18_MMM_1.txt\n",
      "Saved: news_articles\\2025-02-18_MMM_2.txt\n",
      "Saved: news_articles\\2025-02-18_MMM_3.txt\n",
      "OHLC Data Fetched: MMM - Close: 148.6199951171875\n",
      "\n",
      "Fetching news for: CAT...\n",
      "Saved: news_articles\\2025-02-17_CAT.txt\n",
      "Saved: news_articles\\2025-02-18_CAT.txt\n",
      "Saved: news_articles\\2025-02-18_CAT_1.txt\n",
      "Saved: news_articles\\2025-02-18_CAT_2.txt\n",
      "Saved: news_articles\\2025-02-18_CAT_3.txt\n",
      "OHLC Data Fetched: CAT - Close: 353.32000732421875\n",
      "\n",
      "Fetching news for: LMT...\n",
      "Saved: news_articles\\2025-02-15_LMT.txt\n",
      "Saved: news_articles\\2025-02-18_LMT.txt\n",
      "Saved: news_articles\\2025-02-11_LMT.txt\n",
      "Saved: news_articles\\2025-01-28_LMT.txt\n",
      "Saved: news_articles\\2025-02-14_LMT.txt\n",
      "OHLC Data Fetched: LMT - Close: 423.19000244140625\n",
      "\n",
      "Fetching news for: RTX...\n",
      "Saved: news_articles\\2025-02-15_RTX.txt\n",
      "Saved: news_articles\\2025-02-18_RTX.txt\n",
      "Saved: news_articles\\2025-02-18_RTX_1.txt\n",
      "Saved: news_articles\\2025-02-15_RTX_1.txt\n",
      "Saved: news_articles\\2025-02-15_RTX_2.txt\n",
      "OHLC Data Fetched: RTX - Close: 122.41000366210938\n",
      "\n",
      "Fetching news for: NOC...\n",
      "Saved: news_articles\\2025-02-17_NOC.txt\n",
      "Saved: news_articles\\2025-02-18_NOC.txt\n",
      "Saved: news_articles\\2025-02-13_NOC.txt\n",
      "Saved: news_articles\\2025-02-17_NOC_1.txt\n",
      "Saved: news_articles\\2025-02-14_NOC.txt\n",
      "OHLC Data Fetched: NOC - Close: 438.8999938964844\n",
      "\n",
      "Fetching news for: DE...\n",
      "Saved: news_articles\\2025-02-17_DE.txt\n",
      "Saved: news_articles\\2025-02-14_DE.txt\n",
      "Saved: news_articles\\2025-02-17_DE_1.txt\n",
      "Saved: news_articles\\2025-02-18_DE.txt\n",
      "Saved: news_articles\\2025-02-18_DE_1.txt\n",
      "OHLC Data Fetched: DE - Close: 480.2200012207031\n",
      "\n",
      "Fetching news for: AMZN...\n",
      "Saved: news_articles\\2025-02-17_AMZN.txt\n",
      "Saved: news_articles\\2025-02-17_AMZN_1.txt\n",
      "Saved: news_articles\\2025-02-17_AMZN_2.txt\n",
      "Saved: news_articles\\2025-02-17_AMZN_3.txt\n",
      "Saved: news_articles\\2025-02-17_AMZN_4.txt\n",
      "OHLC Data Fetched: AMZN - Close: 228.67999267578125\n",
      "\n",
      "Fetching news for: TSLA...\n",
      "Saved: news_articles\\2025-02-17_TSLA.txt\n",
      "Saved: news_articles\\2025-02-15_TSLA.txt\n",
      "Saved: news_articles\\2025-02-15_TSLA_1.txt\n",
      "Saved: news_articles\\2025-02-14_TSLA.txt\n",
      "Saved: news_articles\\2025-02-14_TSLA_1.txt\n",
      "OHLC Data Fetched: TSLA - Close: 355.8399963378906\n",
      "\n",
      "Fetching news for: HD...\n",
      "Saved: news_articles\\2025-02-17_HD.txt\n",
      "Saved: news_articles\\2025-02-17_HD_1.txt\n",
      "Saved: news_articles\\2025-02-15_HD.txt\n",
      "Saved: news_articles\\2025-02-18_HD.txt\n",
      "Saved: news_articles\\2025-02-18_HD_1.txt\n",
      "OHLC Data Fetched: HD - Close: 409.5\n",
      "\n",
      "Fetching news for: MCD...\n",
      "Saved: news_articles\\2025-02-16_MCD.txt\n",
      "Saved: news_articles\\2025-02-18_MCD.txt\n",
      "Saved: news_articles\\2025-02-16_MCD_1.txt\n",
      "Saved: news_articles\\2025-02-18_MCD_1.txt\n",
      "Saved: news_articles\\2025-02-13_MCD.txt\n",
      "OHLC Data Fetched: MCD - Close: 308.54998779296875\n",
      "\n",
      "Fetching news for: NKE...\n",
      "Saved: news_articles\\2025-02-14_NKE.txt\n",
      "Saved: news_articles\\2025-02-11_NKE.txt\n",
      "Saved: news_articles\\2025-02-15_NKE.txt\n",
      "Saved: news_articles\\2025-02-14_NKE_1.txt\n",
      "Saved: news_articles\\2025-02-18_NKE.txt\n",
      "OHLC Data Fetched: NKE - Close: 73.04000091552734\n",
      "\n",
      "Fetching news for: SBUX...\n",
      "Saved: news_articles\\2025-02-18_SBUX.txt\n",
      "Saved: news_articles\\2025-02-17_SBUX.txt\n",
      "Saved: news_articles\\2025-02-18_SBUX_1.txt\n",
      "Saved: news_articles\\2025-02-17_SBUX_1.txt\n",
      "Saved: news_articles\\2025-02-18_SBUX_2.txt\n",
      "OHLC Data Fetched: SBUX - Close: 112.55000305175781\n",
      "\n",
      "Fetching news for: BKNG...\n",
      "Saved: news_articles\\2025-02-18_BKNG.txt\n",
      "Saved: news_articles\\2025-02-15_BKNG.txt\n",
      "Saved: news_articles\\2025-02-15_BKNG_1.txt\n",
      "Saved: news_articles\\2025-02-17_BKNG.txt\n",
      "Saved: news_articles\\2025-02-18_BKNG_1.txt\n",
      "OHLC Data Fetched: BKNG - Close: 5044.39990234375\n",
      "\n",
      "Fetching news for: LOW...\n",
      "Saved: news_articles\\2025-02-18_LOW.txt\n",
      "Saved: news_articles\\2025-02-17_LOW.txt\n",
      "Saved: news_articles\\2025-02-18_LOW_1.txt\n",
      "Saved: news_articles\\2025-02-16_LOW.txt\n",
      "Saved: news_articles\\2025-02-14_LOW.txt\n",
      "OHLC Data Fetched: LOW - Close: 251.7899932861328\n",
      "\n",
      "Fetching news for: TGT...\n",
      "Saved: news_articles\\2025-02-12_TGT.txt\n",
      "Saved: news_articles\\2025-02-18_TGT.txt\n",
      "Saved: news_articles\\2025-02-18_TGT_1.txt\n",
      "Saved: news_articles\\2025-02-04_TGT.txt\n",
      "Saved: news_articles\\2025-02-18_TGT_2.txt\n",
      "OHLC Data Fetched: TGT - Close: 127.87999725341797\n",
      "\n",
      "Fetching news for: GM...\n",
      "Saved: news_articles\\2025-02-18_GM.txt\n",
      "Saved: news_articles\\2025-02-18_GM_1.txt\n",
      "Saved: news_articles\\2025-02-18_GM_2.txt\n",
      "Saved: news_articles\\2025-01-28_GM.txt\n",
      "Saved: news_articles\\2025-01-28_GM_1.txt\n",
      "OHLC Data Fetched: GM - Close: 48.369998931884766\n",
      "\n",
      "OHLC data saved to 'stock_ohlc_data_ex.xlsx'\n",
      "\n",
      "Past 21 days OHLC data saved to 'Stock_ohlc_past_21_days.xlsx'\n",
      "\n",
      "News scraping and OHLC data fetching completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode, urlparse, parse_qs\n",
    "from datetime import datetime, timedelta\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Stocks to scrape news for\n",
    "stocks = [\n",
    "    \"XOM\", \"CVX\", \"COP\", \"SLB\", \"EOG\", \"OXY\", \"VLO\", \"MPC\", \"PSX\",\n",
    "    \"DOW\", \"DD\", \"LIN\", \"APD\", \"SHW\", \"NEM\", \"FCX\", \"ALB\", \"IP\", \"ECL\",\n",
    "    \"GE\", \"HON\", \"UNP\", \"BA\", \"MMM\", \"CAT\", \"LMT\", \"RTX\", \"NOC\", \"DE\",\n",
    "    \"AMZN\", \"TSLA\", \"HD\", \"MCD\", \"NKE\", \"SBUX\", \"BKNG\", \"LOW\", \"TGT\", \"GM\"\n",
    "]\n",
    "\n",
    "GOOGLE_NEWS_URL = \"https://www.google.com/search\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "OUTPUT_DIR = \"news_articles\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "FETCH_LIMIT = 15\n",
    "MAX_VALID_ARTICLES = 5\n",
    "\n",
    "\n",
    "def fetch_full_article(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def clean_google_url(google_url):\n",
    "    parsed_url = urlparse(google_url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    return query_params.get(\"q\", [google_url])[0]\n",
    "\n",
    "\n",
    "def convert_relative_date(relative_date):\n",
    "    now = datetime.today()\n",
    "    if \"hour\" in relative_date:\n",
    "        num = int(relative_date.split()[0])\n",
    "        article_date = now - timedelta(hours=num)\n",
    "    elif \"day\" in relative_date:\n",
    "        num = int(relative_date.split()[0])\n",
    "        article_date = now - timedelta(days=num)\n",
    "    elif \"week\" in relative_date:\n",
    "        num = int(relative_date.split()[0])\n",
    "        article_date = now - timedelta(weeks=num)\n",
    "    elif \"minute\" in relative_date:\n",
    "        article_date = now\n",
    "    else:\n",
    "        article_date = now\n",
    "    return article_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def scrape_news(stock):\n",
    "    print(f\"\\nFetching news for: {stock}...\")\n",
    "    valid_articles = []\n",
    "    params = {\"q\": f\"{stock} stock news\", \"tbm\": \"nws\", \"hl\": \"en\", \"gl\": \"us\"}\n",
    "    url = f\"{GOOGLE_NEWS_URL}?{urlencode(params)}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch news for {stock}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles_fetched = 0\n",
    "\n",
    "    for article in soup.select(\".SoaBEf\"):\n",
    "        if articles_fetched >= FETCH_LIMIT:\n",
    "            break\n",
    "\n",
    "        title_element = article.select_one(\".nDgy9d\")\n",
    "        url_element = article.select_one(\"a\")\n",
    "        date_element = article.select_one(\".OSrXXb\")\n",
    "\n",
    "        title = title_element.text.strip() if title_element else \"No Title\"\n",
    "        url = url_element[\"href\"] if url_element else \"\"\n",
    "        raw_date = date_element.text.strip() if date_element else \"Unknown Date\"\n",
    "\n",
    "        if url.startswith(\"/url?\"):\n",
    "            url = clean_google_url(url)\n",
    "\n",
    "        full_article = fetch_full_article(url)\n",
    "        if not full_article.strip():\n",
    "            continue\n",
    "\n",
    "        valid_articles.append({\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"date\": convert_relative_date(raw_date),\n",
    "            \"full_article\": full_article,\n",
    "        })\n",
    "\n",
    "        articles_fetched += 1\n",
    "\n",
    "        if len(valid_articles) >= MAX_VALID_ARTICLES:\n",
    "            break\n",
    "\n",
    "    return valid_articles\n",
    "\n",
    "\n",
    "def save_articles(stock, articles):\n",
    "    saved_files = []\n",
    "\n",
    "    for i, article in enumerate(articles):\n",
    "        filename = f\"{article['date']}_{stock}.txt\"\n",
    "        filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "        # Handle duplicate file names (appending _1, _2, etc.)\n",
    "        counter = 1\n",
    "        while os.path.exists(filepath):\n",
    "            filepath = os.path.join(OUTPUT_DIR, f\"{article['date']}_{stock}_{counter}.txt\")\n",
    "            counter += 1\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Stock: {stock}\\n\")\n",
    "            f.write(f\"Date: {article['date']}\\n\")\n",
    "            f.write(f\"Title: {article['title']}\\n\")\n",
    "            f.write(f\"URL: {article['url']}\\n\")\n",
    "            f.write(f\"Full Article:\\n{article['full_article']}\\n\")\n",
    "\n",
    "        saved_files.append(filepath)\n",
    "        print(f\"Saved: {filepath}\")\n",
    "\n",
    "    return len(saved_files) > 0\n",
    "\n",
    "\n",
    "def fetch_recent_ohlc(stock):\n",
    "    try:\n",
    "        ticker = yf.Ticker(stock)\n",
    "        df = ticker.history(period=\"1d\")\n",
    "\n",
    "        if not df.empty:\n",
    "            data = {\n",
    "                \"Stock\": stock,\n",
    "                \"Open_Price\": df.iloc[-1][\"Open\"],\n",
    "                \"Close_Price\": df.iloc[-1][\"Close\"],\n",
    "                \"High_Price\": df.iloc[-1][\"High\"],\n",
    "                \"Low_Price\": df.iloc[-1][\"Low\"],\n",
    "                \"Volume\": df.iloc[-1][\"Volume\"],\n",
    "            }\n",
    "            print(f\"OHLC Data Fetched: {stock} - Close: {data['Close_Price']}\")\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"No OHLC data found for {stock}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch OHLC data for {stock}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_past_21_days_ohlc(stock):\n",
    "    try:\n",
    "        ticker = yf.Ticker(stock)\n",
    "        df = ticker.history(period=\"21d\")\n",
    "\n",
    "        if not df.empty:\n",
    "            df[\"Stock\"] = stock\n",
    "            df.reset_index(inplace=True)\n",
    "            df[\"Date\"] = df[\"Date\"].dt.tz_localize(None)  # Remove timezone\n",
    "            df = df[[\"Date\", \"Stock\", \"Open\", \"High\", \"Close\", \"Low\", \"Volume\"]]\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"No past 21 days OHLC data found for {stock}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch past 21 days OHLC data for {stock}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "all_ohlc_data = []\n",
    "past_21_days_ohlc_data = []\n",
    "\n",
    "for stock in stocks:\n",
    "    # 1. Scrape and Save News Articles\n",
    "    articles = scrape_news(stock)\n",
    "    saved_articles = save_articles(stock, articles) if articles else False\n",
    "\n",
    "    # 2. Fetch OHLC Data only if news was saved\n",
    "    if saved_articles:\n",
    "        ohlc_data = fetch_recent_ohlc(stock)\n",
    "        if ohlc_data:\n",
    "            all_ohlc_data.append(ohlc_data)\n",
    "\n",
    "        # 3. Fetch past 21 days OHLC Data\n",
    "        past_21_days_df = fetch_past_21_days_ohlc(stock)\n",
    "        if not past_21_days_df.empty:\n",
    "            past_21_days_ohlc_data.append(past_21_days_df)\n",
    "\n",
    "    # Pause between requests\n",
    "    time.sleep(random.uniform(5, 10))\n",
    "\n",
    "# Save OHLC Data to Excel\n",
    "if all_ohlc_data:\n",
    "    ohlc_df = pd.DataFrame(all_ohlc_data)\n",
    "    ohlc_df.to_excel(\"stock_ohlc_data.xlsx\", index=False)\n",
    "    print(\"\\nOHLC data saved to 'stock_ohlc_data_ex.xlsx'\")\n",
    "else:\n",
    "    print(\"\\nNo OHLC data to save.\")\n",
    "\n",
    "# Save Past 21 Days OHLC Data to Excel\n",
    "if past_21_days_ohlc_data:\n",
    "    past_21_days_df = pd.concat(past_21_days_ohlc_data, ignore_index=True)\n",
    "    past_21_days_df.to_excel(\"stock_ohlc_past_21_days.xlsx\", index=False)\n",
    "    print(\"\\nPast 21 days OHLC data saved to 'Stock_ohlc_past_21_days.xlsx'\")\n",
    "else:\n",
    "    print(\"\\nNo past 21 days OHLC data to save.\")\n",
    "\n",
    "print(\"\\nNews scraping and OHLC data fetching completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f13138aa-fa24-49c5-8aae-330726e90df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid date format: Thurs., Feb. 13, 2025 Time: 8:00 AM Eastern time Dial-in (U.S.): +1 800-590-8290 Dial-in (International): +1 240-690-8800 Passcode: ALBQ4\n",
      "Skipping file due to invalid date: 2025-02-13_ALB.txt\n",
      "Output saved to filtered_output1.xlsx\n",
      "Final output with OHLC data saved to 'final_output_with_ohlc.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from datetime import datetime\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def perform_sentiment_analysis(article_text):\n",
    "    vader_scores = analyzer.polarity_scores(article_text)\n",
    "    return vader_scores['compound']\n",
    "\n",
    "def parse_date(date_string):\n",
    "    formats = ['%Y-%m-%d', '%d-%m-%Y', '%d/%m/%Y']\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_string, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    print(f\"Invalid date format: {date_string}\")\n",
    "    return None\n",
    "\n",
    "def assign_dynamic_weights(group):\n",
    "    max_date = group['Date'].max()\n",
    "    group['Time_Diff_Days'] = (max_date - group['Date']).dt.days\n",
    "\n",
    "    min_weight = 0.1\n",
    "    max_weight = 0.9\n",
    "    decay_factor = 0.5\n",
    "\n",
    "    group['Weight'] = max_weight * np.exp(-decay_factor * group['Time_Diff_Days'])\n",
    "    group['Weight'] = group['Weight'].clip(lower=min_weight, upper=max_weight)\n",
    "    group['Weight'] = group['Weight'] / group['Weight'].sum()\n",
    "\n",
    "    return group\n",
    "\n",
    "def process_files(input_folder, output_excel, ohlc_file, final_output_with_ohlc):\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(input_folder, filename), 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "                stock = date = title = url = None\n",
    "                full_article_lines = []\n",
    "                collecting_article = False\n",
    "\n",
    "                for line in lines:\n",
    "                    if line.startswith(\"Stock:\"):\n",
    "                        stock = line.split(\":\", 1)[1].strip()\n",
    "                    elif line.startswith(\"Date:\"):\n",
    "                        date = line.split(\":\", 1)[1].strip()\n",
    "                    elif line.startswith(\"Title:\"):\n",
    "                        title = line.split(\":\", 1)[1].strip()\n",
    "                    elif line.startswith(\"URL:\"):\n",
    "                        url = line.split(\":\", 1)[1].strip()\n",
    "                    elif line.startswith(\"Full Article:\"):\n",
    "                        collecting_article = True\n",
    "                    elif collecting_article:\n",
    "                        full_article_lines.append(line.strip())\n",
    "\n",
    "                full_article = \"\\n\".join(full_article_lines).strip()\n",
    "\n",
    "                if not full_article or len(full_article.split()) < 5:\n",
    "                    print(f\"Skipping file due to empty or short article: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                sentiment_score = perform_sentiment_analysis(full_article)\n",
    "\n",
    "                date = parse_date(date)\n",
    "                if not date:\n",
    "                    print(f\"Skipping file due to invalid date: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                if stock and date and title and url:\n",
    "                    data.append({\n",
    "                        'Date': date,\n",
    "                        'Stock': stock,\n",
    "                        'Title': title,\n",
    "                        'URL': url,\n",
    "                        'Sentiment_score': sentiment_score\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Skipping file due to missing fields: {filename}\")\n",
    "\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.sort_values(by=[\"Stock\", \"Date\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "        final_results = []\n",
    "\n",
    "        for stock, group in df.groupby('Stock'):\n",
    "            group = group.head(5).copy()\n",
    "            group = assign_dynamic_weights(group)\n",
    "\n",
    "            weighted_avg_score = (group['Sentiment_score'] * group['Weight']).sum()\n",
    "            sentiment_std = group['Sentiment_score'].std() or 0\n",
    "\n",
    "            if weighted_avg_score >= sentiment_std:\n",
    "                final_label_std = 'Positive'\n",
    "            elif weighted_avg_score <= -sentiment_std:\n",
    "                final_label_std = 'Negative'\n",
    "            else:\n",
    "                final_label_std = 'Neutral'\n",
    "\n",
    "            if weighted_avg_score > 0.4:\n",
    "                final_label_threshold = 'Positive'\n",
    "            elif weighted_avg_score < -0.4:\n",
    "                final_label_threshold = 'Negative'\n",
    "            else:\n",
    "                final_label_threshold = 'Neutral'\n",
    "\n",
    "            for _, row in group.iterrows():\n",
    "                final_results.append({\n",
    "                    'Date': row['Date'],\n",
    "                    'Stock': stock,\n",
    "                    'Title': row['Title'],\n",
    "                    'URL': row['URL'],\n",
    "                    'Positive_score': max(row['Sentiment_score'], 0),\n",
    "                    'Negative_score': abs(min(row['Sentiment_score'], 0)),\n",
    "                    'Sentiment_score': row['Sentiment_score'],\n",
    "                    'Weightage': row['Weight'],\n",
    "                    'Weighted_average_score': weighted_avg_score,\n",
    "                    'Final_sentiment_label_on_std': final_label_std,\n",
    "                    'Final_sentiment_label_on_threshold_value': final_label_threshold\n",
    "                })\n",
    "\n",
    "        final_df = pd.DataFrame(final_results)\n",
    "        final_df['Date'] = final_df['Date'].dt.strftime('%d %b %Y %H:%M')\n",
    "        final_df.to_excel(output_excel, index=False)\n",
    "\n",
    "        print(f\"Output saved to {output_excel}\")\n",
    "\n",
    "        # Load OHLC Data\n",
    "        ohlc_df = pd.read_excel(ohlc_file)\n",
    "\n",
    "        # Merge with Analysis Results on 'Stock' only\n",
    "        merged_df = pd.merge(final_df, ohlc_df, on='Stock', how='left')\n",
    "\n",
    "        # Fill missing values for each stock by forward and backward fill\n",
    "        merged_df[['Open_Price', 'Close_Price', 'High_Price', 'Low_Price', 'Volume']] = merged_df.groupby('Stock')[\n",
    "            ['Open_Price', 'Close_Price', 'High_Price', 'Low_Price', 'Volume']\n",
    "        ].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "        merged_df.to_excel(final_output_with_ohlc, index=False)\n",
    "        print(f\"Final output with OHLC data saved to '{final_output_with_ohlc}'\")\n",
    "\n",
    "    else:\n",
    "        print(\"No valid data to process.\")\n",
    "\n",
    "input_folder = 'news_articles'\n",
    "output_excel = 'filtered_output1.xlsx'\n",
    "ohlc_file = 'stock_ohlc_data.xlsx'\n",
    "final_output_with_ohlc = 'final_output_with_ohlc.xlsx'\n",
    "\n",
    "process_files(input_folder, output_excel, ohlc_file, final_output_with_ohlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701ca189-a440-4cbe-bc66-049038dba719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91815\\AppData\\Local\\Temp\\ipykernel_9992\\3263989602.py:63: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Stock', group_keys=False).apply(process_stock)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import talib\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_excel('stock_ohlc_past_21_days.xlsx')\n",
    "\n",
    "# Ensure Date is datetime type and sort by Stock and Date\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.sort_values(by=['Stock', 'Date'], inplace=True)\n",
    "\n",
    "# Indicators, Swing Highs/Lows, Stop Loss, Take Profit per stock\n",
    "def process_stock(stock_df):\n",
    "    # Indicators\n",
    "    stock_df['SMA9'] = talib.SMA(stock_df['Close'], timeperiod=9)\n",
    "    stock_df['SMA21'] = talib.SMA(stock_df['Close'], timeperiod=21)\n",
    "    stock_df['RSI14'] = talib.RSI(stock_df['Close'], timeperiod=14)\n",
    "\n",
    "    # Buy/Sell Signal\n",
    "    stock_df['Buy_Signal'] = (stock_df['SMA9'] > stock_df['SMA21']) & (stock_df['RSI14'] > 50)\n",
    "    stock_df['Sell_Signal'] = (stock_df['SMA9'] < stock_df['SMA21']) & (stock_df['RSI14'] < 50)\n",
    "\n",
    "    # Detect Swing Highs/Lows\n",
    "    def detect_swing_high(stock_df, window=3):\n",
    "        stock_df['Swing_High'] = np.nan\n",
    "        for i in range(window, len(stock_df) - window):\n",
    "            if stock_df['High'].iloc[i] == max(stock_df['High'].iloc[i - window: i + window + 1]):\n",
    "                stock_df.loc[stock_df.index[i], 'Swing_High'] = stock_df['High'].iloc[i]\n",
    "        return stock_df\n",
    "\n",
    "    def detect_swing_low(stock_df, window=3):\n",
    "        stock_df['Swing_Low'] = np.nan\n",
    "        for i in range(window, len(stock_df) - window):\n",
    "            if stock_df['Low'].iloc[i] == min(stock_df['Low'].iloc[i - window: i + window + 1]):\n",
    "                stock_df.loc[stock_df.index[i], 'Swing_Low'] = stock_df['Low'].iloc[i]\n",
    "        return stock_df\n",
    "\n",
    "    stock_df = detect_swing_high(stock_df)\n",
    "    stock_df = detect_swing_low(stock_df)\n",
    "\n",
    "    # Fill NAs in Swing High/Low\n",
    "    stock_df['Swing_High'] = stock_df['Swing_High'].ffill()\n",
    "    stock_df['Swing_Low'] = stock_df['Swing_Low'].ffill()\n",
    "\n",
    "    # Add Stop Loss based on Swing Levels\n",
    "    stock_df['Buy_Stop_Loss'] = stock_df.apply(lambda x: x['Swing_Low'] if x['Buy_Signal'] else np.nan, axis=1)\n",
    "    stock_df['Sell_Stop_Loss'] = stock_df.apply(lambda x: x['Swing_High'] if x['Sell_Signal'] else np.nan, axis=1)\n",
    "\n",
    "    # Risk-to-Reward Ratio for Take Profit\n",
    "    RRR = 2  # Example: Risk-to-Reward Ratio of 2:1\n",
    "\n",
    "    # Add Take Profit levels\n",
    "    stock_df['Buy_Take_Profit'] = stock_df.apply(\n",
    "        lambda x: x['Close'] + (x['Close'] - x['Buy_Stop_Loss']) * RRR if x['Buy_Signal'] else np.nan, axis=1\n",
    "    )\n",
    "    stock_df['Sell_Take_Profit'] = stock_df.apply(\n",
    "        lambda x: x['Close'] - (x['Sell_Stop_Loss'] - x['Close']) * RRR if x['Sell_Signal'] else np.nan, axis=1\n",
    "    )\n",
    "\n",
    "    return stock_df\n",
    "\n",
    "# Group by Stock and process each stock separately\n",
    "df = df.groupby('Stock', group_keys=False).apply(process_stock)\n",
    "\n",
    "# Extract the most recent data for each stock\n",
    "latest_data = df.loc[df.groupby('Stock')['Date'].idxmax()]\n",
    "\n",
    "# Save full data and latest data\n",
    "df.to_excel('output_with_swing_high_low_take_profit.xlsx', index=False)\n",
    "latest_data.to_excel('output_most_recent_data.xlsx', index=False)\n",
    "\n",
    "print(\"Processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a279b6c5-8f93-41a9-a9e3-0c1d210b5558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output with indicators merged successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the final sentiment analysis output\n",
    "analyzer_df = pd.read_excel('final_output_with_ohlc.xlsx')\n",
    "\n",
    "# Load the most recent stock data with indicators\n",
    "indicators_df = pd.read_excel('output_most_recent_data.xlsx')\n",
    "\n",
    "# Select only necessary columns from indicators data\n",
    "indicator_columns = [\n",
    "    'Stock', 'SMA9', 'SMA21', 'RSI14', 'Buy_Signal', 'Sell_Signal',\n",
    "    'Swing_High', 'Swing_Low', 'Buy_Stop_Loss', 'Sell_Stop_Loss',\n",
    "    'Buy_Take_Profit', 'Sell_Take_Profit'\n",
    "]\n",
    "\n",
    "indicators_df = indicators_df[indicator_columns]\n",
    "\n",
    "# Merge the two dataframes based on 'Stock'\n",
    "merged_df = pd.merge(analyzer_df, indicators_df, on='Stock', how='left')\n",
    "\n",
    "# Save the merged dataframe to a new Excel file\n",
    "merged_df.to_excel('final_output_with_indicators.xlsx', index=False)\n",
    "\n",
    "print(\"Final output with indicators merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab81757-eb74-4255-8f48-d3e423ccb230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
